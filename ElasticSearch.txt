ElasticSearch
-	OpenSource Analytics and full Search Engine

Usages
	-	Query and analyze structured data
	-	Analyse application logs and system metrics
	-	Application performance management
	-	Send events to elasticsearch
	-	Forecast future values with machine learning
	-	Anomality detection

How does ElasticSearch work?
	-	Data is stored as documents. Similar to rows in SQL.
	-	A document's data is separated into fields. Similar to
		columns in relational databases.
	-	Written in Java, build on Apache lucene.
	-	Easy to use, and highly scalable.

ElasticSearch Stack
	-	Kibana
		-	An analytics and visualisation platform.
	-	Beats
		-	A collection of data shippers that send data to Elasticsearch or Logstash.
	-	LogsStash
		-	A data processing pipeline
	-	X-Pack
		-	Adds additional features to Elasticsearch and kibana.

		

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
✅ Elasticsearch security features have been automatically configured!
✅ Authentication is enabled and cluster connections are encrypted.

ℹ️  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
  LkcIgwxHsiO-ch7OfdL*

ℹ️  HTTP CA certificate SHA-256 fingerprint:
  6a0b4a461f8bfcec1efddfcb0561b1acd62004bd40bf87d61b2285c0078380b8

ℹ️  Configure Kibana to use this cluster:
• Run Kibana and click the configuration link in the terminal when Kibana starts.
• Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
  eyJ2ZXIiOiI4LjYuMiIsImFkciI6WyIxOTIuMTY4LjAuNTY6OTIwMCJdLCJmZ3IiOiI2YTBiNGE0NjFmOGJmY2VjMWVmZGRmY2IwNTYxYjFhY2Q2MjAwNGJkNDBiZjg3ZDYxYjIyODVjMDA3ODM4MGI4Iiwia2V5IjoiT0htNERJY0IzY21Xd1pKRmJpYm06U1JGTms3MWhTbU9keno4U1VLVHhaUSJ9

ℹ️  Configure other nodes to join this cluster:
• On this node:
  ⁃ Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`.
  ⁃ Uncomment the transport.host setting at the end of config/elasticsearch.yml.
  ⁃ Restart Elasticsearch.
• On other nodes:
  ⁃ Start Elasticsearch with `bin/elasticsearch --enrollment-token <token>`, using the enrollment token that you generated.


elastic password - LjqY3C+_53nh01LRrP0H

reset password - bin/elasticsearch-reset-password -u elastic
eyJ2ZXIiOiI4LjYuMiIsImFkciI6WyIxOTIuMTY4LjAuNTU6OTIwMCJdLCJmZ3IiOiJhZTc2ZTU3NzU0NzMzZGQ2ZDllYTY3OTA5MzQ2ZGE1ZTMwOTFhNDIyNTkwOGI0MTAwY2JkOWRkNzc4NDI3YTQ4Iiwia2V5IjoianpuUjlZWUJrOUVtUGNGZFNjbHo6bzVvS04yS2RUZGU5T3hMWUpybnFXQSJ9
kibana enrollment token - 
new enrolment token for kibana - bin/elasticsearch-create-enrollment-token --scope kibana



Basic Architecture Summary
	-	Nodes store the data that we add to elasticsearch.
	-	A cluster is a collection of nodes.
	-	Data is stored as documents, which are Json objects.
	-	Documents are grouped together with indices.


What is index in elasticSearch?
	-	An index is a logical namespace that holds a collection of documents with similar characteristics. It is similar to a database table in a traditional relational database management system(RDBMS).


Sharding and Scalability
	-	Sharding is a way to divide indices into smaller pieces.
	-	Each piece is called as shard.
	-	Sharding is done at the index level and not cluster or node level.
	-	The main purpose is to horizontally scale the data volume.

Let's dive deeper
	-	A shard is an independent index... kind of.
	-	Each shard is an Apache lucene index.
	-	An ElasticSearch index consists of one or more Lucene indices.
	-	A shard has no predefined size; it grows as documents are
		added to it.
	-	A shard may store up to 2 billion documents.
The purpose of Sharding
	-	Mainly be able to store more documents
	-	To easier fit large indices onto nodes.
	-	Improved performance.
		-	Parallelization of queries increases the throughput of an index.

Configuring the number of shards
	-	An index contains single shard by default. See the pri field when running GET _cat/indices?v&expand_wildcards=all
	-	Also, while creating an index, it creates by default 1 primary shard and 1 replica shard.
		If elastic is running on single node only, then the replica shard is unassigned. In that case
		cluster health will be yellow instead of green.
	-	Indices in elasticSearch < 7.0.0 were created with five shards
		-	This often led to oversharding.
	-	Increase the number of shards using the Split API in new elasticSearch.
	-	Reduce the number of shards with the Shrink API.
	-	Once you set the number of shards for an index in ElasticSearch, you cannot change them. You will need to create a new index with the desired number of shards, and depending on your use case, you may want then to transfer the data to the new index

Introduction to Replication.
	-	What happens if a node's hard drive fails?
	-	Hardware can fail at any given time, so we need to handle that somehow
	-	ElasticSearch supports fault tolerance using replication.
	-	Replication is supported natively and enabled by default.


How does replication work?
	-	Replication is configured at index level.
	-	Replication works by creating copies of shards, referred to as replica shards.
	-	A shard that has been replicated, is called primary shard.
	-	A primary shard and its replica shards are referred to as 
		a replication group.
	-	Replica shards are complete copy of a shard.
	-	A replica shard can serve search requests, exactly like its primary shard.
	-	The number of replicas can be configured at index creation.
	-	Example :
		-	Primary Shard A, Replica A1, Replica A2 <- Replication Group A
		-	Primary Shard B, Replica B1, Replica B2 <- Replication Group B
		- The above two collectively form an index.
	- But what if entire disk stops working and we loose all of the data?
	- To avoid this replica shards are not created on the same node of their primary shard.
	- For example Node A will contain primary Shard A, replica shard B1 and replica shard B2.
	And Node B will contain primary Shard B, replica shard A1 and replica shard A2.

Snapshots
	-	Elasticsearch supports taking snapshots as backups.
	-	Snapshots can be used to restore to a given point in time.
	-	Snapshots can be taken at the index level, or for the entire cluster.
	-	Use snapshots as backups and replication for high availability(and performance).
	-	Replications work on live data, but what if live data gets corrupted. In that case we need to rollback in which
	snapshots provides the backups.
	-	Replications also helps in increasing the throughput.
		If there are multiple requests then elasticsearch will delegate the requests to the shards.


Adding more nodes to cluster
	-	Sharding enables us to scale an index data volume
	-	But eventually we will need to add additional nodes
	-	Also, replication requires at least two nodes.
	-	We will try to add two more nodes to our cluster. This can be done in local setup only.
	-	To make nodes first try to extract the elastic archive. If you want to create two nodes then
		make two copies. Make sure not to copy the existing node's directory. Because it contains data used by that node.
	-	For each node extract the archive and create.
	-	Now rename each node by going to /config/application.yml.
	-	First time whenever we start a node we need to specify the enrollment token. Using enrollment token helps elasticsearch to use the configurations that we don't need to do.
	-	First go to the existing node folder and create the enrollment token using
		bin/elasticsearch-create-enrollment-token -s node
	-	copy the enrollment token and then start the other nodes using this command
		bin/elasticsearch --enrollment-token <enrollment-token>
	Word of Caution
		-	After adding third node, atleast two nodes are required to run the cluster
			-	If two nodes are lost, the cluster cannot elect a master node.
		-	If you want to run a single node moving forward, don't add the third node.
			-	Alternatively you can start fresh with a single-node cluster afterwards.


Overview of Node roles
	-	Master-eligible
		-	The node may be elected as the cluster's master node.
		-	A master node is responsible for creating and deleting indices, among others.
		-	A node with this role will not automatically become the master node.
			-	unless there are no other master-eligible nodes
		-	May be used for having dedicated master nodes. Because Master node should not be busy in doing other things, such as serving requests. Also, if master node is doing other things then there may be high cpu consumption, memory consumption.
			-	Useful for large clusters

	-	Data Node
		-	Enables a node to store data.
		-	Storing data includes performing queries related to that data, such as search queries
		-	For relatively small clusters, this role is almost always enabled.
		-	Useful for having dedicated master master nodes.
		-	Used as part of configuring a dedicated master node.

	-	Ingest Nodes
		-	Enables a node to run ingest pipelines.
		-	Ingest pipelines are a series of steps(processors) that are performed when indexing documents.
			-	Processors may manipulate documents, eg. resolving an IP to lat/lon
		-	A simplified version of Logstash, directly within Elasticsearch.
		-	This role is mainly useful for having dedicated ingest nodes.

	-	Machine Learning Node
		-	node.ml identifies a node as a machine learning node.
			-	This lets the node run machine learning jobs
		-	xpack.ml.enabled enables or disables the machine learning API for the node.
		-	Useful for running ML jobs that don't affect other tasks.

	-	Coordination Node
		-	Coordination refers to the distribution of queries and the aggregation of results.
		-	Useful for coordination-only nodes(for large clusters)
		-	Configured by disabling all other roles.

	-	Voting Node
		-	Rarely used, and you almost certainly won't use it either.
		-	A node with this role, will participate in the voting for a new master node.
		-	The node cannot be elected as the master node itself, though
		-	Only used for large clusters.


Creating and Deleting Indices
	-	DELETE /pages
	-	PUT /products
	-	PUT /products
		{
			"settings" : {
				"number_of_shards" : 2,
				"number_of_replicas" : 2
			}
		}
	-	Here there will be only 1 primary shard and the other two replica shards will be identical to
		its primary shard. Thereby forming a replication group. so when data is inserted it will be inserted in the 3 shards.
	-	Modifying new indices requires reindexing docs to new index.	

Inserting Docs
	-	POST /products/_doc
		{
			"name" : "Coffee Maker",
			"price": 64,
			"in_stock": 10
		}
		-	This will insert a document with random generated index and will be copied to the 3 shards.
	-	PUT /products/_doc/100
		{
			"name" : "Toaster",
			"price": 55,
			"in_stock": 4
		}
		-	Since we have inserted the first doc, now to add more docs we will use PUT instead of POST.
		-	Also here we are specifying the id value as 100.
		-	POST is usually used for automatic id creation whereas for PUT add new doc by providing an id.

Retrieving Docs
	-	GET /products/_doc/100
		-	Here if the document exists then, found flag will be true and the doc will show under _source flag.
		-	Otherwise, found flag will be false and _source flag will not be present.


Updating Docs
	-	POST /products/_update/100
		{
			"doc" : {
				"in_stock" : 3
			}
		}
	-	Adding fields to the same doc can be done using the same above query
	-	POST /products/_update/100
		{
			"doc" : {
				"tags" : ["electronics"]
			}
		}

Documents are immutable
	-	Elasticsearch documents are immutable (!)
	-	We actually replaced documents
	-	The update API did some things for us, making it look like we updated documents
	-	The Update API is simpler and saves some network traffic.

How the update API works
	-	The current document is retrieved.
	-	The field values are changed.
	-	The existing document is replaced with modified document.
	-	We could do the exact same thing at application level.


Scripted Updates
	-	POST /products/_update/100
		{
			"script" : {
				"source" : "ctx._source.in_stock--"
			}
		}
		-	subtracts the instock value by 1.
	-	POST /products/_update/100
		{
			"scripts" : {
				"source" : "ctx._source.in_stock = 10"
			}
		}

	-	POST /products/_update/100
		{
			"script" : {
				"source" : "ctx._source.in_stock -= params.quantity",
				"params" : {
					"quantity" : 4
				}
			}
		}

	-	POST /products/_update/100
		{
			"script" : {
				"source": """
					if (ctx._source.in_stock > 0) {
						ctx._source.in_stock--;
					}
				"""
			}
		}


Upserting Documents
	-	If the document does not already exist, the contents of the upsert element are inserted as a new document. If the document exists, the script is executed.
	-	POST /products/_update/101
		{
			"script" : {
				"source" : "ctx._source.in_stock++"
			},
			"upsert" : {
				"name" : "Blender",
				"price" : 399,
				"in_stock" : 5
			}
		}
	-	If the document is not present then the result keyword will have created as value, otherwise it will have updated as value


Replacing Documents
	-	PUT /products/_doc/101
		{
			"name" : "Toaster",
			"price": 79,
			"in_stock": 4
		}


Deleting Documents
	-	DELETE /products/_doc/101


Introduction to Routing
	-	How does ElasticSearch know where to store documents ?
	-	How are documents found once they have been indexed ?
	-	The answer is routing 
	-	Routing is the process of resolving a shard for a document.
	-	It is resolved by using the formula.
		-	shard_num = hash(_routing) % num_primary_shards.
	-	Same formula is used when retrieving/updating/deleting the document.
	-	When you index a document, Elasticsearch will determine which shard the document should be routed to for indexing. Where the default value of _routing is _id.
	-	When using custom routing, in that case _routing value is different.



Custom Routing
	-	Routing is 100% transparent when using ElasticSearch.
	-	This makes Elasticsearch easier to use and learn.
	-	It is possible to customize routing, though, for various purposes.
	-	The default routing strategy ensures that the documents are distributed evenly.

Interesting fact
	-	As we know we cannot change the number of shards after the index has been created. This is because
		routing formula depends on the number of shards.


How ElasticSearch Reads data
	-	A read request is received and handled by coordinating node.
	-	Routing is used to resolve the document's replication group.
	-	ARS is used to send the query to the best available shard.
		-	ARS is short for Adaptive Replica Selection.
		-	ARS helps reduce query response time.
		-	ARS is essentially an intelligent load balancer.
	-	The coordinating node collects the response and sends it to the client.


How ElasticSearch Writes Data
	-	Write operations are sent to primary shards.
	-	The primary shard forwards the operation to replica shards.
	-	Primary terms and sequence numbers are used to recover from failures.
	-	Primary term
		-	Counter denoting how many times a Primary shard has been elected.
		-	Initially the counter is set to 1, when a failure occurs and a new primary shard
			has been elected. In that case, counter is increment to 2.
	-	Sequence Number
		-	Counter for each write operation and is only incremented by Primary shard.
	-	Global and local checkpoints help speed up the recovery process.
	-	Global Checkpoint
		-	Checkpoint for every replication group. Basically minimum sequence number that is common in replication group.
	-	Local CheckPoint
		-	Checkpoint for every replica shard. Basically maximum sequence number out of all the replica shards.
	-	Primary term and sequence numbers are available within responses.

Introduction to Versioning
	-	Not a revision history of documents.
	-	Elasticsearch stores an _version metadata field with every document.
		-	The value is an integer
		-	It is incremented by one when modifying a document.
		-	The value is retained for 60 seconds when deleting a document.
			-	Configured with the index.gc_deletes setting.
		-	The _version field is returned when retrieving documents.

	-	Types of Versioning
		-	The default versioning type is called internal versioning.
		-	There is also an external versioning type 
			-	Useful when versions are maintained outside of ElasticSearch.
			-	Eg: When documents are also stored in RDBMS

What is the point of Versioning?
	-	You can tell how many times document has been modified.
		-	Probably not that useful

	-	Versioning is hardly used anymore, and is mostly a thing from the past.
	-	It was previously the way to do optimistic concurrency control.
		-	Now there is better way using primary terms and sequence numbers.
	-	This _version field is being used for clusters running old versions.


Optimistic Concurrency Control
	-	Prevent overwriting documents inadvertently due to concurrent operations.
	-	There are many scenarios in which this can happen
		-	handling concurrent visitors for web application.


How do we handle failures?
	-	Handle the situation at application level.
		-	Retrieve the document again.
		-	Use _primary_term and _seq_no for new update request.
		-	Remember to perform any calculations that use field values again.


Summary Concurrency Control
	-	Sending write requests to ElasticSearch concurrently may overwrite changes
		made by other concurrent processes.
	-	Traditionally, the _version field was used to prevent this
	-	Today, we use the _primary_term and _seq_no fields.
	-	Elasticsearch will reject a write operation if it contains the wrong primary term or sequence number
		-	This should be handled at application level.



Update by query
	-	Problem - To decrease the in_stock field by one for every products.
	-	POST /products/update_by_query
		{
			"script" : {
				"source" : "ctx._source.in_stock--"
			}
			"query" : {
				"match_all" : {}
			}
		}
	-	This will update the stock for all products. Here we are specifying constraints using
		query keyword.

	-	If a document has been modified since taking the snapshot, the query is aborted.
		-	This is checked using document's primary term and sequence number.
	-	To count version conflicts instead of aborting the query, the conflicts option
		can be set to proceed.


Delete by query
	-	To delete all the documents
	-	POST /products/delete_by_query
		{
			"query" : {
				"match_all" : {

				}
			}
		}


Batch Processing
	-	POST /_bulk
		{ "index" : { "_index" : "products", "_id" : 200}}
		{"name" : "Espresso Machine", "price" : 199, "in_stock" : 5}
		{"create" : {"_index" : "products", "_id" : 201}}
		{"name" : "Milk Frother", "price" : 149, "in_stock" : 14}

	-	here index and create is a type of action that we want to perform. Diff is that create action will give an error if the document already exists, but index will replace the document if it already exists. If doc is not present in that case it will be inserted for both the action type
	-	2nd and 4th line describes the doc information that we want to store.

	-	POST /_bulk
		{ "update" : { "_index" : "products", "_id" : 200}}
		{ "doc" : {"price" : 129}}
		{ "delete" : { "_index" : "products", "_id" : 200}}

	-	Or you can use this if all the operations are on same index
		POST /products/_bulk
		{ "update" : {"_id" : 200}}
		{ "doc" : {"price" : 129}}
		{ "delete" : {"_id" : 200}}



Analysis
	-	Sometimes referred to as text analysis
	-	Applicable to text field/values.
	-	Text values are analyzed when indexing documents.
	-	The result is stored in data structures that are efficient for searching etc.
	-	The _source object is not used when searching for documents
		-	It contains the exact values specified when indexing documents.
	-	When a text value is indexed, a so-called analyzer is used to process the text.
	-	An analyzer contains of three building blocks;
		-	character filters
		-	tokenizer
		-	token filters
    -   The result of analyzing text values is then stored in a searchable data structure.
    
    -   Character Filters
        -   Adds, removes or changes characters.
        -   Analyzer contains zero or more character filters.
        -   Character filters are applied in the order in which they are specified.
        -   Example (html_strip) filter
            -   Input: "I&apos;m in a <em>good</em> mood&nbsp;-&nbsp; and I<strong>love</strong> acai"
            -   Output: "I am in a good mood and I love acai"   

    -   Tokenizer
        -   An analyzer contains one tokenizer
        -   Tokenizes a string, i.e splits it into tokens.
        -   Characters may be stripped as a part of the tokenization.
        -   Example: 
            Input : "I REALLY like beer!"
            Output : ["I", "REALLY", "like", "beer"]
        -   Although not seen here, tokenizer also records the character offsets in the original string.

    -   Token Filters
        -   Receive the output of the tokenizer as input(i.e the tokens)
        -   A token filter can add, remove, or modify tokens
        -   An analyzer contains zero or more token filters.
        -   Token filters are applied in the order they are specified.
        -   Example(lowercase filter)
            -   Input: ["I", "REALLY", "like", "beer"]
            -   Output: ["i", "really", "like", "beer"]

    -   These three building blocks collectively make a standard analyser.
    -   The analyser is used for all text fields unless configured otherwise.
    -	Specifying the standard analyzer explicitly in the request.
    -   POST /_analyze/
		{
  			"text": "2 guys walk into  a bar, but the third... DUCKS! :-)",
  			"analyzer": "standard"
		}
	-	Response
		{
          "tokens": [
            {
              "token": "2",
              "start_offset": 0,
              "end_offset": 1,
              "type": "<NUM>",
              "position": 0
            },
            {
              "token": "guys",
              "start_offset": 2,
              "end_offset": 6,
              "type": "<ALPHANUM>",
              "position": 1
            },
            {
              "token": "walk",
              "start_offset": 7,
              "end_offset": 11,
              "type": "<ALPHANUM>",
              "position": 2
            },
            {
              "token": "into",
              "start_offset": 12,
              "end_offset": 16,
              "type": "<ALPHANUM>",
              "position": 3
            },
            {
              "token": "a",
              "start_offset": 18,
              "end_offset": 19,
              "type": "<ALPHANUM>",
              "position": 4
            },
            {
              "token": "bar",
              "start_offset": 20,
              "end_offset": 23,
              "type": "<ALPHANUM>",
              "position": 5
            },
            {
              "token": "but",
              "start_offset": 25,
              "end_offset": 28,
              "type": "<ALPHANUM>",
              "position": 6
            },
            {
              "token": "the",
              "start_offset": 29,
              "end_offset": 32,
              "type": "<ALPHANUM>",
              "position": 7
            },
            {
              "token": "third",
              "start_offset": 33,
              "end_offset": 38,
              "type": "<ALPHANUM>",
              "position": 8
            },
            {
              "token": "ducks",
              "start_offset": 42,
              "end_offset": 47,
              "type": "<ALPHANUM>",
              "position": 9
            }
          ]
        }

    -   In above you can see elastic is smart enough to remove commas, exclamation marks and whitespaces also. This is done because they provide no value
        when performing text searches.
    -   Instead of specifying the analyzer we can also specify the parts of the analyzer like this but the result will be the same as mentioned above.
    -   POST /_analyze/
        {
          "text": "2 guys walk into  a bar, but the third... DUCKS! :-)",
          "char_filter": [],
          "tokenizer": "standard",
          "filter": ["lowercase"]
        }

UnderStanding Inverted Indices
    -   A field's value is stored in several data structures.
        -   The data structure depends on the field's dataType.
    -   Ensure efficient data access    -   e.g searches.
    -   Handled by Apache Lucene, not ElasticSearch.
    -   One such data structure is Inverted Index.

    -   Inverted Indices
        -   Mapping between terms and which documents contain them
        -   terms/tokens here are the output of the tokenizer.
        -   terms are sorted alphabetically
        -   Inverted indices contain more than just terms and document IDs
            -   Example of inverted index.
            -   Text field in Document 1 - "2 guys walk into a bar, but the third... DUCKS! :-)"
            -   Text field in Document 2 - "2 guys went into a bar"
            -   Text field in Document 3 - "2 ducks walk around the lake"
            -   Now for each of the text, it will be analyzed by the standard analyser and will be converted into tokens.
            -   Now a table will be created for the sorted tokens(observed in the documents) and the documents.
            -   Each entry in the table will show whether this token is present in a given document or not.

            -   This is an example showing that how inverted indices are created for a single text field present in the document.
            -   What happens if there are multiple text fields in the document? In that case inverted index is created for each text field.
            -   So, if a document has two text field then 2 inverted indices are created.
        -   One inverted index per text field
        -   Other data types use BKD trees as data structure.

Introduction to Mapping
    -   What is mapping?
        -   Defines the structure of documents and how they are stored. Eg. field and their datatypes
        -   Similar to table schema in a relational database.
        -   Types of Mapping
            -   Explicit Mapping    -   We define field mappings ourselves.
            -   Dynamic Mapping     -   ElasticSearch will generate field mapping automatically.   


Overview of DataTypes
    -   Some of the basic ElasticSearch data types.
    -   object, integer, long, boolean, double, text, short, float and date.
    -   There are some specialised data types such as ip data types to store ip addresses.
    
    -   Object datatype
        -   Used for any JSON object
        -   Objects may be nested
        -   Mapped using properties parameter
        -   {
                "name: : "Coffee maker",
                "price" :  64.2,
                "in_stock" : 10,
                "is_active" : true,
                "manufacturer" : {
                    "name" : "Nespresso",
                    "country" : "Switzerland"
                }
            }

        -   PUT /products
            {
                "mappings" : {
                    "properties" : {
                        "name" : {"type" : "text"},
                        "price" : {"type" : "double"},
                        "in_stock": {"type" : "short"},
                        "is_active" : {"type" : "boolean"},
                        "manufacturer" : {
                            "properties" : {
                                "name" : {"type" : "text"},
                                "country" : {"type" : "text"}
                            }
                        }
                    }
                }
            }

        -   Objects are not stored as objects in Apache lucene.
            -   Objects are transformed to ensure that we can index any valid json.
            -   In particular, objects are flattened.
        -   The above example will be flattened as below
            -   {
                    "name" : "Coffee maker",
                    "price" : 64.2,
                    "in_stock" : 10,
                    "is_active" : true,
                    "manufacturer.name" : "Nespresso",
                    "manufacturer.country" : "Switzerland"
                }

        -   Now what if there are array of fields. In that case it will create something like this.
            -   {
                    "manufacturer.name" : ["Nespresso", "Nescafe"],
                    "manufacturer.country" : ["Switzerland", "India"]
                }
            -   It becomes difficult, to search documents like this. When running the below query, elastic won't be able to diffrentiate
                which manufacturer country belongs to which manufacturer name
            -   Query : Match products where manufacturer.name == "Nescafe" AND manufacturing.country == "India".


    -   nested data type
    	-   Similar to the object data type, but maintains objects relationship.
        -   Useful when indexing array of objects.
        -   Enables us to query objects independently. meaning object values are not mixed together as in the above example of array objects.
            -   Must use the nested query.
            -   {
                    "name" : "Coffee maker",
                    "reviews" : [
                        {
                            "rating" : 5.0,
                            "author" : "Average Joe",
                            "description" : "Haven't slept for days... Amazing!"
                        },
                        {
                            "rating" : 3.5,
                            "author" : "John Doe",
                            "description" : "Could be better"
                        }
                    ]
                }

            -   PUT /products
                {
                    "mappings" : {
                        "properties" : {
                            "name" : {"type" : "text"},
                            "reviews": {"type" : "nested"}
                        }
                    }
                }
            -   Query : Match products where reviews.author == "John Doe" AND reviews.rating >= 4.0
            -   This query now works correctly because the objects are stored independently and the query works correctly.
        -   nested objects are stored as hidden documents.
        -   Suppose that we index a document containing product info. and that document has a reviews array field which have ten review objects inside it.
            This would cause to index 11 documents into lucene, one for the product and 10 for the reviews.


    -   Keyword datatype
        -   Used for exact matching of values.
        -   Typically used for filtering, aggregations and sorting.
        -   E.g. searching for articles with the status of "PUBLISHED".
        -   for full text searches, use the text datatype instead. full text searches are the searches that do not require exact matches.
            -   Eg. searching the body text of an article 

    -	How the keyword datatype works
    	-	How keyword fields are analyzed
    		-	keyword fields are analyzed with keyword analyzer
    		-	The keyword analyzer is a no-op analyzer
    			-	It outputs the unmodified string as a single token.
                -   This token is then placed into the inverted index.
    			-	POST /_analyze
    				{
    					"text" : "2 guys walk into a bar, but the third... DUCKS! :-)",
    					"analyzer" : "keyword"
    				}
    			-	Response
    				{
                      "tokens": [
                        {
                          "token": "2 guys walk into a bar, but the third... DUCKS! :-)",
                          "start_offset": 0,
                          "end_offset": 51,
                          "type": "word",
                          "position": 0
                        }
                      ]
                    }

                -   As you can see the string is untouched and returned as it is as a single token. If symbols were to be stripped out, for instance, 
                    we wouldn't be able to perform exact searches.
                -   Why, because the values within the inverted index (in the form of term/tokens) would then differ from the string that was provided with the indexed document.
                    Searching for the original value would therefore yield no results.
                -   what it would look like if we if save the full text as a token/term.
                    TERM                                    Document1   Document2   Document3
                    2 ducks walk around the lake            X
                    2 guys walk into a bar, but                         X
                    the third... DUCKS! :-)                     
                    2 guys went into a bar.                                         X

                -   This is unlike the text fields where the standard analyzer is used.
                -   Also keyword analayzer does not removes symbols and lower case letters.
                -   Much more realistic example when keyword analyzer is used is to search emailAddresses
            -   keyword fields are used for exact matching, aggregations and sorting.

Introduction to type coercion
	-	Data types are inspected when indexing documents.
		-	They are validated, and some invalid values are rejected.
		-	E.g trying to index an object for a text field.
	-	Sometimes, providing the wrong data type is okay
	-	Eg.-
		-	PUT /coercion_test/_doc/1
			{
				"price" : 7.4
			}
		-	PUT /coercion_test/doc/2
			{
				"price": "7.4"
			}
		-	PUT /coercion_test/doc/3
			{
				"price" : "7.4m"
			}
		-	GET /coercion_test/_doc/2
		-	coercion_test index is yet not created. It will be created when the first document is inserted.
		-	When the document is inserted then a dynamic mapping of datatype is created for the price field. In this case it will be float.
		-	What ElasticSearch did was to inspect the value that we supplied in order to figure out its datatype. In this case we supplied a floating point number, so the float data type was used within the mapping.
		-	In our second doc a floating point number is represented but in the form of string. If we run the query then it will also indexed. We can see that for the same price field in first doc we are passing a float value and in another float value as a string.
		-	This is where type coercion comes into the picture.
		-	Elasticsearch first resolves the data type that was supplied for a given field. In case there is a mapping then the two data types are compared.
		-	Since we supplied a string for a numeric data type, ElasticSearch inspects the string value.
		-	If it contains a numeric value and only numeric value Elasticsearch will convert the string to
			the correct data type, being float in this example.
		-	The result is that a string of 7.4 is converted into the floating point number 7.4 instead, and the document is indexed as if we had supplied a number in the first place.
		-	Now, if we try to pass a string that cannot be converted to float like the third document above, in that case it will give exception that it cannot be converted.
		-	Also if you try to get the second document it will return string "7.4" and not float 7.4 .
			But, as mentioned above that ElasticSearch coerces the value into a float instead of a string. So, why this behaviour ?
		-	The reason is that the "_source" key contains the original values that we indexed.
		-	UnderStanding the _source object
			-	Contains the values that are supplied at index time in our case it is("7.4")
				-	Not the values that are indexed(converted to 7.4 via coercion)
			-	Search queries use indexed values, not _source values.
				-	BKD trees, inverted indices etc.
				-	Elasticsearch searches the data structure used by Apache Lucene to store the field values.
				-	In case of text fields, that would be an inverted index.
				-	In this example, the value has been converted into a float, even though we see a string.
				-	Within apache lucene the value is stored as a numeric value and not a string.
			-	_source does not reflect how values are indexed
				-	Keep coercion in mind if you use values from _source.
		-	Coercion is not used for dynamic mapping.
			-	Supplying "7.4" for a new field will create a text mapping.
		-	Always try to use the correct datatype.
			-	Especially the first time you index a field.
		-	Disabling coercion is a matter of preference.
			-	It is enabled by default.



Introduction to arrays
	-	There is no such thing as array data type.
	-	Any field may contain zero or more values.
		-	No configuration or mapping needed.
		-	Simply supply an array when indexing document.
		-	POST /products/_doc
			{
				"tags" : "Smartphone"
			}
			POST /products/_doc
			{
				"tags" : ["Smartphone", "Electronics"]
			}
		-	The above two queries are valid and can be run in any order.
		-	The mapping that will be created is as follows
			{
				"products" : {
					"mapping" : {
						"properties" : {
							"tags" : {
								"type" : "text"
							}
						}
					}
				}
			}
		-	As we can see there is no sign of array data type in the mapping.
		-	So how does this get stored ?
		-	In the case of text fields, the strings are simply concatenated before being analyzed and
			the resulting token is stored within the inverted index as normal.
		-	POST /_analyze
			{
				"text" : ["Strings are simply", "merged together"],
				"analyzer" : "standard"
			}
			POST /_analyze
			{
				"text" : ["Strings",  "are", "simply", "merged",  "together"],
				"analyzer" : "standard"
			}
		-	If your run the above two analyze api then you would see that it is returning the same result.
			It means that the text values in array are stored as a same string.
		-	When storing the strings are concatenated with a space in between, because otherwise
			the word simply and merged would end up within the same token.
		-	In case of non-text fields the values are not analyzed, multiple values are just stored within
			the appropriate data structure within Apache lucene.


		-	Constraints
			-	Array values should be of the same datatype. This means you cannot mix strings and integers.
			-	You can mix datatypes together only if passed datatype can be coerced with the mapping datatype.
			-	This is true for simple datatypes but not for the other ones such as objects.
				[{"name" : "Coffee Maker"}, {"name" : "Toaster"}, false]
				-	here false cannot be converted to object type.
			-	Coercion only works for fields that are already mapped.
				-	If creating a field mapping with dynamic mapping, an array must contain the same data type. Mixed data types are not allowed until the mapping is present. Mixed data type array can be passed after the mapping is created.

		-	Nested arrays
			-	Arrays may contain nested arrays
			-	Arrays are flattened during indexing.
			-	[1, [2, 3]] becomes [1, 2, 3].

Adding Explicit Mappings
	-	PUT /reviews
		{
			"mappings" : {
				"properties" : {
					"rating" : {
						"type" : "float"
					},
					"content" : {
						"type" : "text"
					},
					"product_id" : {
						"type" : "integer"
					},
					"author" : {
						"properties" : {
							"first_name" : {
								"type" : "text"
							},
							"last_name" : {
								"type" : "text"
							},
							"email" : {
								"type" : "keyword"
							}
						}
					}
				}
			}
		}
	-	Now adding documents
		PUT /reviews/_doc/1
		{
			"rating" : 5.0,
			"content" : "Outstanding course! Bo really taught me a lot about ElasticSearch!",
			"product_id" : 123,
			"author" : {
				"first_name" : "John",
				"last_name" : "Doe",
				"email" : "johndoe123@example.com"
			}
		}

	-	This above document will get inserted using the respective mappings.
	-	But if we change the value of email field from string to an {} denoting an object. Then elasticSearch won't be able to coerce the value.
	-	Also, instead of string if we change the field value of email to integer then elasticSearch will be able to coerce the value.

Retrieving Mappings
	-	To retrieve a value of given index. Just query the mapping api
	-	GET /reviews/_mapping
	-	To retrieve the mapping of a particular field, 
	-	GET /reviews/_mapping/field/content  where content is the field name.
	-	GET /reviews/_mapping/field/author.email where author.email is nested field.
                    

Using dot notations in field names
	-	the above explicit mapping of author object can be done in a simpler way
		-	PUT /reviews
		{
			"mappings" : {
				"properties" : {
					"rating" : {
						"type" : "float"
					},
					"content" : {
						"type" : "text"
					},
					"product_id" : {
						"type" : "integer"
					},
					"author.firstname" : {
						"type" : "text"
					},
					"author.lastName" : {
						"type" : "text"
					},
					"author.email" : {
						"type" : "keyword"
					}
				}
			}
		}

Adding mappings to existing indices
	-	To add new mappings to existing index, we will be using the mapping api.
	-	Since we are using the mapping api we don't need to add "mapping" keyword in the request body
	-	Instead just use the properties to define mapping for a new field.
	-	PUT /reviews/_mapping
		{
			"properties" : {
				"created_at" : {
					"type" : "date"
				}
			}
		}
	-	GET /reviews/_mapping

Introduction to Dates
	-	Specified in one of three ways.
		-	Specially formatted strings.
		-	Milliseconds since the epoch (long)
		-	Seconds since the epoch(integer)
	-	Epoch refers to 1st of January of 1970
	-	Custom date formats are supported.

	-	Default Behaviour of Date fields
		-	Three supported formats:
			-	A date without time
			-	A date with time
			-	Milliseconds since the epoch (long)

		-	In case of string timestamps UTC timezone is assumed if nothing is specified.
		-	If we supply a string value for the date value, the date needs to be in ISO 8601 specification
	-	How date fields are stored
		-	Stored internally as milliseconds since the epoch (long)
		-	Any valid value that you supply at the index time is converted to a long value internally.
		-	Dates are converted to UTC timezone
		-	The same date conversion also happen for date queries, too. This date will be converted to long before the search process begins.
	-	Example
		when only date is specified for date field. Elastic Search assumes time to be midnight of that particular day and will convert them to milliseconds since the epoch.
		PUT /reviews/_doc/2
		{
			"rating" : 4.5,
			"content" : "Not bad. Not bad at all",
			"product_id" : 123,
			"created_at" : "2015-03-27",
			"author" : {
				"first_name" : "Average",
				"last_name" : "Joe",
				"email" : "avgjoe@example.com"
			}
		}


		when date with time is specified for date field. Here the date is specified in utc timezone
		using the z.
		PUT /reviews/_doc/3
		{
			"rating" : 4.5,
			"content" : "Not bad. Not bad at all",
			"product_id" : 123,
			"created_at" : "2015-03-27T13:07:41Z",
			"author" : {
				"first_name" : "Average",
				"last_name" : "Joe",
				"email" : "avgjoe@example.com"
			}
		}

		when date with time is specified for date field.
		PUT /reviews/_doc/4
		{
			"rating" : 4.5,
			"content" : "Not bad. Not bad at all",
			"product_id" : 123,
			"created_at" : "2015-03-28T09:21:51+01:00",
			"author" : {
				"first_name" : "Average",
				"last_name" : "Joe",
				"email" : "avgjoe@example.com"
			}
		}

		when date with time is specified but in epoch. Here the epoch should be in milliseconds since epoch
		PUT /reviews/_doc/5
		{
			"rating" : 4.5,
			"content" : "Not bad. Not bad at all",
			"product_id" : 123,
			"created_at" : "1436011284000",
			"author" : {
				"first_name" : "Average",
				"last_name" : "Joe",
				"email" : "avgjoe@example.com"
			}
		}


		To get all the queries
		The values are the exact values that we specified when indexing the documents, and they do not
		represent how dates are stored for searching
		GET /reviews/_search
		{
			"query" : {
				"match_all" : {}
			}
		}

Handling Missing Fields.
	-	what happens when we don't provide field values during indexing a document.
	-	Fields can have zero or more values by default it means all fields are optional.
	-	You can leave a field out when indexing documents.
	-	E.g unlike relational databases where you need to allow null values.
	-	Some integrity checks need to be done at the application level
		-	E.g having required fields.
	-	Adding a field mapping does not make a field required. You can leave the field value optional with mapping present.
	-	Searches automatically handle missing fields. In other words, you don't need to actively handle
		if some documents may have left fields out.


Overview of Mapping Parameters
	-	format parameter
		-	Used to customize the format for the date fields
		-	Default date format is recommended for use.
			-	"strict_date_optional_time || epoch_millis"
		-	Using Java's DateFormatter syntax
			-	Eg "dd/MM/yyyy"
		-	Using built-in formats
			-	E.g "epoch_seconds"
		-	Eg:
			PUT /sales
			{
				"mappings" : {
					"properties" : {
						"purchased_at" : {
							"type" : "date",
							"format" : "dd/MM/yyyy"
						}
					}
				}
			} 


			PUT /sales
			{
				"mappings" : {
					"properties" : {
						"purchased_at" : {
							"type" : "date",
							"format" : "epoch_second"
						}
					}
				}
			}
	-	properties parameter
		-	Defines nested fields for object and nested fields.
		-	Examples
			PUT /sales
			{
				"mappings" : {
					"properties" : {
						"sold_by" : {
							"properties" : {
								"name" : {
									"type" : "text"
								}
							}
						}
					}
				}
			}

	-	coerce parameter
		-	Used to enable or disable coercion values (enabled by default)
		-	PUT /sales
			{
				"mappings" : {
					"properties" : {
						"amount" : {
							"type" : "float",
							"coerce" : false
						}
					}
				}
			}
		-	It is possible to enable and disable coercion at index level. So you don't have to specify
			coercion parameter for every parameter/field.
			PUT /sales
			{
				"settings" : {
					"index.mapping.coerce" : false
				},
				"mappings" : {
					"properties" : {
						"amount" : {
							"type" : "float",
							"coerce": true
						}
					}
				}
			}
			-	Here coerce is disabled at index level. but gets overridden by the amount field



	-	Introduction to doc_values
		-	Elasticsearch makes use of several data structures.
			-	No single datastructure serves all purposes.
		-	Inverted indices are excellent for searching text.
			-	They don't perform well for many other data access patterns.
		-	"Doc values" is another data structure used by Apache lucene.
			-	Optimized for a different data access pattern (document-> terms)
			-	inverted index does not perform well in case of sorting or aggregating results because the access pattern is different. Instead doc_values perform well in this case.
			-	Instead of looking terms and finding the documents that contain them, we need to look up the document and find its terms for a field.

		-	doc_values is opposite of inverted index, i.e. an uninverted inverted index.
		-	Used for sorting, aggregations and scripting.
		-	It is an additional data structure and not a replacement for inverted index. Both can be applied to a field at the same time.
		-	Elasticsearch automatically queries the appropriate data structure.
		-	Disabling doc_values 
			-	Set the doc_values parameter to false to save disk space.
			-	Storing data in multiple data structures effectively duplicates data with the purpose of fast retrieval
				-	Also slightly increasing the index throughput
			-	Only disable doc values if you won't use aggregations, sorting and scripting.
			-	Particularly useful for large indices, typically not worth it for small ones.
			-	Cannot be changed without reindexing documents into new index.
				-	Use with caution, and try to anticipate how fields will be queried
			-	Example
				PUT /sales
				{
					"mappings" : {
						"properties" : {
							"buyer_email" : {
								"type" : "keyword",
								"doc_values": false
							}
						}
					}
				}
	-	Norms parameter
		-	Normalization factors used for relevance scoring.
		-	Often we don't just want to filter results, but also rank them based on how well they match a given query. Think of search results on Google, for instance.
		-	Norms can be disabled to save disk space. Because they also take a lot of disk space like doc_values.
			-	Useful for fields that won't be used for relevance scoring.
			-	If disabled then also fields can be used for filtering and aggregations.
			-	Example
				PUT /sales
				{
					"mappings" : {
						"properties" : {
							"tags" : {
								"type" : "text",
								"norms" : false
							}
						}
					}
				}
	-	index parameter
		-	Disables indexing for a field.
		-	Values are still stored within _source object. It is just that the field value won't be part for the data structures used for searching.
		-	Useful if you won't use a field for search queries
		-	Saves disk space and slightly improves indexing throughput.
		-	Often used for time series data
		-	Fields with indexing disabled can still be used for aggregations.

	-	null_value parameter
		-	null values cannot be indexed or searched.
		-	Use this parameter to replace NULL values with another value so that the field value can be searched.
		-	Only works for explicit NULL values. It won't work for example array field with empty array.
		-	The replacement value must be of the same data type as the field.
		-	Does not affect the value stored within _source.
		-	PUT /sales
			{
				"mappings" : {
					"properties" : {
						"paramter_id" : {
							"type" : "keyword",
							"null_value" : "NULL"
						}
					}
				}
			}
	-	copy_to parameter
		-	Used to copy multiple field values into a "group field". If a doc has two field first_name and last_name and we want to query for the full name. In that case we can copy the two fields into a single field full_name using copy_to.
		-	Simply specify the name of the target field as the value.
		-	E.g first_name and last_name -> full_name
		-	Values are copied, not terms/tokens
			-	The analyzer of the target field is used for the values.
		-	The target field is not a part of _source
		-	Example
			PUT /sales
			{
				"mappings" : {
					"properties" : {
						"first_name": {
							"type" : "text",
							"copy_to" : "full_name"
						},
						"last_name" : {
							"type" : "text",
							"copy_to" : "full_name"
						},
						"full_name" : {
							"type" : "text"
						}
					}
				}
			}

			POST /sales/_doc
			{
				"first_name" : "John",
				"last_name" : "Doe"
			}

Updating Existing Mappings
	-	Suppose that product IDs may now include letters.
	-	We need to change the product_id field's data type to either text or keyword
		-	We won't use the field for full-text searches
		-	We will use it for filtering, so the keyword datatype is ideal.
	-	If we try to update the mapping which is already present using the below query then we get the following message.
		-	PUT /reviews/_mapping
			{
				"properties" : {
					"product_id" : {
						"type" : "keyword"
					}
				}
			}
		-	It will give illegal_argument_exception.
	-	Limitations for updating mappings
		-	Generally, Elasticsearch field mappings cannot be changed.
		-	We can add new field mappings, but that's about it.
		-	A few mapping parameters can be updated for existing mappings.
		-	Example
			PUT /reviews/mapping
			{
				"properties" : {
					"author" : {
						"properties" : {
							"email" : {
								"type" : "keyword",
								"ignore_above" : 256
							}
						}
					}
				}
			}
			The above query will work which will update the existing field mapping for email. It will update such that email greater than 256 letters will be ignored and won't be stored or be a part for searching.
		-	Being able to update mappings would be problematic for existing documents
			-	Text values have already been analyzed, for instance
			-	Changing between some datatypes would require rebuilding the whole data structure.
		-	Even for an empty index, we cannot update a mapping.
		-	Field mappings also cannot be removed.
			-	Just leave out the field when indexing documents.
		-	The Update By Query API can be used to reclaim disk space.
		-	The solution is to reindex documents into a new index.


Reindex documents with the Reindex API
	-	GET /reviews/_mappings
		-	this will retrieve the mappings for the given index.
	-	copy the mappings object and then paste it to the query
	-	changing the keyword datatype from long to keyword.
	-	PUT /reviews_new/
		{
		  "mappings": {
		    "properties": {
		      "author": {
		        "properties": {
		          "email": {
		            "type": "text",
		            "fields": {
		              "keyword": {
		                "type": "keyword",
		                "ignore_above": 256
		              }
		            }
		          },
		          "first_name": {
		            "type": "text",
		            "fields": {
		              "keyword": {
		                "type": "keyword",
		                "ignore_above": 256
		              }
		            }
		          },
		          "last_name": {
		            "type": "text",
		            "fields": {
		              "keyword": {
		                "type": "keyword",
		                "ignore_above": 256
		              }
		            }
		          }
		        }
		      },
		      "content": {
		        "type": "text",
		        "fields": {
		          "keyword": {
		            "type": "keyword",
		            "ignore_above": 256
		          }
		        }
		      },
		      "created_at": {
		        "type": "date"
		      },
		      "product_id": {
		        "type": "keyword"
		      },
		      "rating": {
		        "type": "float"
		      }
		    }
		  }
		}

	-	Now to transfer the documents to the new index ElasticSearch exposes a api called as 
		_reindex api.
	-	POST /_reindex
		{
			"source" : {
				"index" : "reviews"
			},
			"dest" : {
				"index" : "reviews_new"
			}
		}
	-	Now to search for documents
		GET /reviews_new/_search
		{
		  "query" : {
		    "match_all" : {}
		  }
		}
	-	But, if we check the docs in the new index we will see that, product_id is present in the form of long or integer and not string as we added the keyword datatype.
	-	_source datatypes
		-	The data type doesn't reflect how the values are indexed.
		-	_source contains the field original values supplied at index time. In our case it was integer or long. Us changing the data type does not affect the "_source" object to be modified.
		-	It's common to use _source values from search results.
			-	You would probably expect a string for a keyword field.
		-	We can modify the _source value while reindexing.
		-	Alternatively this can be handled at application level.
		-	To update the datatype for the fields inside the source object a script can be written. But before that docs needs to be deleted from the new index.
		-	POST /reviews_new/_delete_by_query
			{
				"query" : {
					"match_all" : {

					}
				}
			}
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews"
				},
				"dest" : {
					"index" : "reviews_new"
				}
				"script" : {
					"source" : """
						if (ctx._source.product_id != null) {
							ctx._source.product_id = ctx._source.product_id.toString();
						}
					"""
				}
			}
		-	If now if you check then the fields inside the _source object has converted from integer/long to string.

	-	Reindex documents matching a query
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews",
					"query" : {
						"match_all" : {}
					}
				},
				"dest" : {
					"index": "reviews_new"
				}
			}
	-	Reindex only positive reviews
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews",
					"query" : {
						"range" : {
							"rating" : {
								"gte" : 4.0
							}
						}
					}
				},
				"dest" : {
					"index" : "reviews_new"
				}
			}
	-	Removing fields
		-	Field mappings cannot be deleted.
		-	Fields can be left out when indexing documents.
		-	Maybe we want to reclaim disk space used by a field.
			-	Already indexed values still take up disk space.
			-	For large data sets, this maybe worthwhile
				-	Assuming that we no longer need the values.
		-	This can be done using the source filtering.
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews",
					"_source" : ["content", "created_at", "rating"]
				},
				"dest" : {
					"index" : "reviews_new"
				}
			}
		-	By specifying an array of field names, only those fields are included for each document when they are indexed into the destination index. In other words, any fields that you leave out will not be reindexed.
	-	Changing a field's name
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews"
				}
				"dest" : {
					"index" : "reviews_new"
				},
				"script" : {
					"source" : """
						#Rename "content" field to "comment"
						ctx._source.comment = ctx._source.remove("content");
					"""
				}
			}
	-	Ignore reviews with ratings below 4.0
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews"
				},
				"dest" : {
					"index" : "reviews_new"
				},
				"script" : {
					"source" : """
						if (ctx._source.rating < 4.0) {
							ctx.op = "noop"; # Can also be set to "delete"
						}
					"""
				}
			}
		-	In case of noop document will not be indexed to the destination index
		-	In case of delete document will get deleted from the destination index.
			-	The destination index might not be empty as in our example. So the document may be present in the destination index. It will get deleted if used "delete".

-	Batching and Throttling
	-	The Reindex API performs operations in batches
		-	Just like the Update by Query and Delete by Query APIs.
		-	It uses the scroll API internally.
		-	This is how millions of documents can be reindexed efficiently.
	-	Throttling can be configured to limit the performance impact.
	-	Check documentation if you need to reindex lots of documents.


-	Defining Field Aliases
	-	Field names can be changed while reindexing documents
		-	But it is not worth it for lot of documents for the sake of renaming.
	-	An alternative is to use field aliases
		-	Doesn't require documents to be reindexed
		-	Aliases can be used within queries
		-	Aliases are defined with a field mapping.
		-	Example, here comment alias is used for field name content
			PUT /reviews/_mapping
			{
				"properties" : {
					"comment" : {
						"type" : "alias",
						"path" : "content"
					}
				}
			}
-	Updating field aliases
	-	Field aliases can actually be updated
		-	Only its target field, though
	-	Simply perform a mapping update with new path value.
	-	This is possible since aliases don't affect indexing.
		-	It's a query level construct used when parsing queries, whether it is search or index request.


-	Index aliases
	-	Similar to field aliases, Elasticsearch also supports index aliases
	-	Typically used when dealing with large data volumes.


-	Multi Field Mappings
	-	A field can be mapped in multiple ways. For instance a "text" field maybe mapped as a 
		"keyword" field at the same time.
	-	Lets say you want to do search query on a particular field in that case you should use "text" as a type for the field. Also, you want to perform aggregations on that same field. But, aggregations don't work with "text" type. They work with only "keyword" field
	-	Before MultiField Mapping example
		-	PUT /multi_field_test
			{
				"mappings" : {
					"properties" : {
						"description" : {
							"type" : "text"
						},
						"ingredients" : {
							"type" : "text"
						}
					}
				}
			}
		-	Here we want to perform search and aggregations on the "ingredients" field. Currently only search will work since it has only "text" as type.
		-	After modifying to use both search and aggregations.
		-	PUT /multi_field_test
			{
				"mappings" : {
					"properties" : {
						"description" : {
							"type" : "text"
						},
						"ingredients" : {
							"type" : "text"
							"fields" : {
								"keyword" : {
									"type" : "keyword"
								}
							}
						}
					}
				}
			}
		-	Here we have added additional multi field mapping for the field "ingredients". This above request adds the "ingredients.keyword" keyword multi-field, which can be used for sorting and aggregations.

		-	POST /multi_field_test/_doc
			{
				"description" : "To make spaghetti, carbonara, you first need to...",
				"ingredients" : ["Spaghetti", "Bacon", "Eggs"]
			}

		-	This creates inverted index for three fields description, ingredients and ingredients.keyword.
		-	description and ingredients are analyzed and converted into inverted index using standard analyzer
		-	ingredients.keyword is converted into inverted index using keyword analyzer where input is unmodified. Input tokens/terms remain as it is which can then be used in aggregations or sorting.

		-	Now to do exact match on keyword type we will run the following query.
			-	GET /multi_field_test/_search
				{
					"query" : {
						"term" : {
							"ingredients.keyword": "Spaghetti"
						}
					}
				}

-	Index Templates
	-	Index templates specify settings and mappings.
	-	They are applied to indices that match one or more patterns
	-	Patterns may include wildcards(*) 
	-	Index templates take effect only when creating new indices
	-	When an index is created either manually or through indexing a document - the template
		settings are used as a basis for creating the index.
	-	We can add a new index template by using the Index template API together with the PUT HTTP verb.
	-	The request path is "_template" followed by the name of index template.
	-	Example
		-	PUT /_template/access-logs
			{
				"mappings" : {
					"properties" : {
						"@timestamp" : {
							"type" : "date"
						},
						"url.original" : {
							"type" : "keyword"
						},
						"http.request.referrer" : {
							"type" : "keyword"
						},
						"http.response.status_code" : {
							"type" : long
						}
					}
				}
			}
		-	This is a template with name "access-logs" because it is going to define mappings for indices storing HTTP access logs
		-	Only one important part is missing i.e for which indices this template should be applied to.
		-	Those should be defined within an "index_patterns" parameters consisting of array of strings.
		-	PUT /_template/access-logs
			{
				"index_patterns" : ["access-log-*"],
				"mappings" : {
					"properties" : {
						"@timestamp" : {
							"type" : "date"
						},
						"url.original" : {
							"type" : "keyword"
						},
						"http.request.referrer" : {
							"type" : "keyword"
						},
						"http.response.status_code" : {
							"type" : long
						}
					}
				}
			}
		-	index patterns are essentially index names where parts of them may be wild cards. You can also use exact names of the indices, but that probably defeats the purpose of using an index template.
		-	Now what is the purpose of this asterisk?
		-	Elasticsearch is often used to store time series data, which can be a lot of different things, just as long as the entries are stored chronologically.
		-	For example, we could capture and store the hardware usage of servers every minute, or perhaps every second.
		-	Or we could ping an api to store uptime information
		-	So why to include wildcard in the index pattern?
		-	Because time series data is often divided into multiple indices. How many depends on the data volume, so it could be per year, month, week, or day, for instance.
		-	A common approach is to create one index per day for large data volumes

	-	Apart from specifying mappings, an index template can also contain index settings. Let's specify the number of shards.
	-	PUT /_template/access-logs
			{
				"index_patterns" : ["access-log-*"],
				"settings" : {
					"number_of_shards" : 2,
					"index.mapping.coerce" : false 
				}
				"mappings" : {
					"properties" : {
						"@timestamp" : {
							"type" : "date"
						},
						"url.original" : {
							"type" : "keyword"
						},
						"http.request.referrer" : {
							"type" : "keyword"
						},
						"http.response.status_code" : {
							"type" : long
						}
					}
				}
			}
	-	Now since index template is created, we will create an index that matches the index pattern of the template.
	-	Specifically, an index to contain access logs for the 1st January, 2020.
	-	PUT /access-logs-2020-01-01 to create the index
	-	GET /access-logs-2020-01-01 to get the contents
		-	Now the response will have all the fields defined using the index template with the necessary index settings
	-	There is one case that, if while creating new index we specify the mappings or settings and the index name matches with the index pattern of the index template. Then settings defined in both will be merged together. In case of duplicates, the configuration from the create index request will take precedence and override the value specified in the index template.

	-	Priorities of index templates
		-	A new index may match multiple index templates
		-	An order parameter can be used to define the priority of index templates
			-	The value is simply an integer
			-	Templates with lower values are merged first.

	-	Updating an index template
		-	An index template can be updated using the same command. Just specify the full the configuration that is to be updated
		-	PUT /_template/access-logs
			{
				#Full configuration
			}
		-	An updation in index templates affects the new indices; existing indices that matched the index template are left untouched.

	-	An index template can be retrieved and deleted using the same endpoint with the GET and DELETE verbs respectively.

-	Introduction to ECS(Elastic Common Schemas)
	-	A specification of common fields and how they should be mapped in Elasticsearch.
	-	Before ECS, there was no cohesion between field names.
	-	Ingesting logs from nginx would give different field names than Apache.
		-	For instance, a Kibana dashboard needed to know which particular web server was being used because the field names were different at least while using filebeat modules.
		-	There was really no standard, and so the fields were often named something along the lines of
			"apache2.access.url" or "ngnix.access.url"
		-	This was inconvenient because the data consumer such as kibana needed to know event source implementation.
		-	However, once the access logs are parsed, it makes no difference if the web server that generated them is named Apache or ngnix.
		-	Changing the webserver would cause all kinds of trouble all the way through ingestion pipeline because field names would then be different.
		-	Therefore we want to keep the field names the same so that kibana doesn't need to know if events come from an Apache or ngnix web server.
		-	It is pretty common to ingest data from number of sources at the same time. For instance we might want to collect data from a number of different services such as a Postgres database, Kafka ngnix, and perhaps a redis cluster.
		-	We would probably process an event once per minute and we would need to store this timestamp within a field.
		-	Instead of having each service potentially name this field differently, the ECS Specification states that it should be named @timestamp.
		-	This means that no matter which kind of event you are dealing with, this should be the name when the event originated.
		-	This makes it easier when consuming data perhaps through kibana because the name of the timestamp field is same regardless of the event source.
	-	ECS means that common fields are named the same thing
		-	E.g. timestamp
		-	ECS was created to overcome the challenges of having different field names for the same things.
	-	Use-case independent.
		-	Naturally ECS is not related to web server logs
	-	Groups of field are referred to as "field sets". This can be found in ECS documentation. Examples are Host fields, Log fields, Network fields etc.
	

-	Uses of ECS
	-	In ECS, "documents" are referred to as events.
		-	ECS doesn't provide fields for non-events(e.g products, employee name, salary etc)
	-	Mostly useful for standard events.
		-	e.g web server logs, operating system metrics, etc
	-	ECS is automatically handled by Elastic Stack products.
		-	Like for example FileBeat or MetricBeat which structures data according to ECS automatically.
		-	If you use them, you often won't have to actively deal with ECS, because it has already been handled for you.
	-	You might not need to use ECS, but it is good to know what it is.


-	Introduction to Dynamic Mapping
	-	POST /my-index/_doc
		{
			"tags" : ["computer", "electronics"],
			"in_stock" : 4,
			"created_at" : "2020/01/01 00:00:00"
		}

	-	creates a dynamic mapping as follows
		{
			"created_at" : {
				"type" : "date",
				"format" : "yyyy/MM/dd HH:mm:ss||yyyy/MM/dd||epoch_millis"
			},
			"in_stock" : {
				"type" : "long"
			},
			"tags" : {
				"type" : "text",
				"fields" : {
					"keyword" : {
						"type" : "keyword",
						"ignore_above" : 256
					}
				}
			}
		}
	-	Here, everything looks simple mapping. But, for tags field it creates two types of mapping
		"text" mapping which will be used for full text searches, while "keyword" mapping should be used for exact matches, aggregations and sorting
	-	This is a default behaviour where it automatically creates the mapping. But it is not always be the best mapping. Since, it will cause optimization issues

	-	Rules for creating dynamic mapping
		json 			ElasticSearch
		string 			One of the following
						-	text field with keyword mapping
						-	date field
						-	(float or long field)

		integer 		long
		float 			float
		boolean			boolean
		object 			object
		array 			Depends on the first non-null value.


-	Configuring Dynamic Mapping
	-	Creating mapping for only one field
		PUT /people
		{
			"mappings" : {
				"dynamic" : false,
				"properties" : {
					"first_name" : {
						"type" : "text"
					}
				}
			}
		}

	-	Adding a doc
		POST /people/_doc 
		{
			"first_name" : "Bo",
			"last_name" : "Anderson"
		}
	-	Getting the mapping information, this will retrieve the above mapping info
		GET /people/_mapping

	-	Searching the doc using first name, it will contain both first_name and last_name field.
		GET /people/_search
		{
			"query" : {
				"match" : {
					"first_name" : "Bo"
				}
			}
		}
	-	Searching the doc using last name, response will be empty.
		GET /people/_search
		{
			"query" : {
				"match" : {
					"last_name" : "Anderson"
				}
			}
		}
	-	Setting dynamic to false
		-	New fields are ignored, but not rejected.
			-	They are not indexed, but still are part of _source
		-	No inverted index is created for the last_name field.
			-	Querying the field gives no results
		-	Fields cannot be indexed without mapping.
			-	When enabled dynamic mapping creates one before indexing values.
		-	New fields must be mapped explicitly if dynamic is set to false, so that they are searchable.
	-	Another Better way
		-	setting dynamic to "strict"
		-	Elasticsearch will reject unmapped fields
			-	All fields must be mapped explicitly.
			-	Similar to the behavior of relational databases.
		-	Creating mapping with dynamic as strict
			-	
				PUT /people
				{
					"mappings" : {
						"dynamic" : false,
						"properties" : {
							"first_name" : {
								"type" : "text"
							}
						}
					}
				}
		-	Now inserting doc with incomplete mapping will result in exception, saying lastname is not mapped
			-	POST /people/_doc 
				{
					"first_name" : "Bo",
					"last_name" : "Anderson"
				}

		-	Inheriting and overriding dynamic field
			PUT /computers
			{
				"mappings" : {
					"dynamic" : "strict",
					"properties" : {
						"name" : {
							"type" : "text"
						},
						"specifications" : {
							"properties" : {
								"cpu" : {
									"properties" : {
										"name" : {
											"type" : "text"
										}
									}
								},
								"other" : {
									"dynamic" : true
									"properties" : {...}
								}
							}
						}
					}
				}
			}
			-	Here at the top level we defined dynamic as strict, so all the other field, name, specifications will also inherit this field. Any other field added while creating the doc
			will result in exception
			-	To override the value, other field has added the value of dynamic as true. In this case any extra field added under other field will have dynamic mapping and it will get indexed.


-	Numeric Detection
	-	PUT /computers
		{
			"mappings" : {
				"numeric_detection" : true
			}
		}
	-	POST /computers/_doc
		{
			"specifications" : {
				"other" : {
					"max_ram_gb" : "32",
					"bluetooth" : "5.2"
				}
			}
		}
	-	When numeric detection is enabled, elasticsearch will check the contents of the string, to see if they only contain numeric values. In that case datatype of the field will be set to either float or long.

-	Disabling Date detection
	-	PUT /computers
		{
			"mappings" : {
				"date_detection" : false
			}
		}
	-	by default elastic search will check if a given string is of a given date format or not. If it is then it will create a date field
	-	To keep the field in string format and not create a date object	set the date_detection to false.

-	Configuring date detection formats
	-	PUT /computers
		{
			"mappings" : {
				"dynamic_date_formats" : ["dd-MM-yyyy"]
			}
		}

-   Dynamic Templates
    -   Another way in which we can configure dynamic mapping is by using so called dynamic templates.
    -   A dynamic templates consists of one or more conditions along with the mapping a field should use if it matches the conditions.
    -   Dynamic templates are used when dynamic mapping is enabled and a new field is encountered without any existing mapping.
    -   Dynamic templates are added within a key named "dynamic_templates" nested with "mappings" key.
    -   each dynamic template should be an object within this array.
    -   Within this object we add a key with the name of the dynamic template, which can be anything we want.
    -   Next we want to define when this template should triggered i.e. a match condition.
    -   Defining a match_mapping_type will define for which datatype will this template be applied.
    -   Define a field mapping that should be used in case the condition matches. This is done inside the mapping parameter.

    -   PUT /dynamic_template_test
        {
            "mappings" : {
                "dynamic_templates" : [
                    {
                        "integers" : {
                            "match_mapping_type" : "long",
                            "mapping" : {
                                "type" : "integer"
                            }
                        }
                    }
                ]
            }
        }
    -   POST /dynamic_template_test/_doc
        {
            "in_stock" : 123
        }
    -   We defined a condition that evaluates to true if the detected json data type is long in which case the field mapping will be
        as specified.
    -   In this example, this means that dynamic field mappings will not use the "long" data type as would otherwise be the default
        behaviour, but the "integer" data type instead.

    -   Another example would be mapping strings where by default ignore_above parameter is 256. This can be changed using dynamic_templates
    -   PUT /test_index
        {
            "mappings" : {
                "dynamic_templates" : [
                    {
                        "strings" : {
                            "match_mapping_type" : "string",
                            "mapping" : {
                                "type" : "text",
                                "fields" : {
                                    "keyword" : {
                                        "type" : "keyword",
                                        "ignore_above" : 512
                                    }
                                }
                            }
                        }
                    }
                ]
            }
        }
    -   This example shows how string values can be mapped according to the default rules, with the exception that the "ignore_above"
        parameter contains a value of 512 instead of 256.

    -   match and unmatched parameters
        -   used to specify conditions for the name of the field being evaluated.
        -   field names must match the condition specified by the match parameter
        -   unmatch field is used to exclude fields that were matched by the match parameter
        -   Both parameters support pattern with wildcards(*)
            -   Hardcoding specific field names wouldn't make any sense

        -   Example
            -   PUT /test_index
                {
                    "mappings" : {
                        "dynamic_templates" : [
                            {
                                "strings_only_text" : {
                                    "match_mapping_type" : "string",
                                    "match" : "text_*",
                                    "unmatch" : "*_keyword",
                                    "mapping" : {
                                        "type" : "text"
                                    }
                                }
                            },
                            {
                                "strings_only_keyword" : {
                                    "match_mapping_type" : "string",
                                    "match" : "*_keyword",
                                    "mapping" : {
                                        "type" : "keyword"
                                    }
                                }
                            }
                        ]
                    }
                }
        -   In this example we have two dynamic templates. In situations where there are more than one, the templates are
            processed in order and the first matching template wins.
        -   The first template matches fields where the name begins with the string "text" and an underscore.
        -   The unmatch parameter then filter out fields that end with an underscore and the string "keyword"
        -   What this template does is therefore to map string values to "text" fields provided the field matches the match
            parameter and does not match the unmatch parameter.

        -   So, if you run this query
            POST /test_index/_doc
            {
                "text_product_description" : "A description.",
                "text_product_id_keyword" : "ABC-123"
            }
            -   The first field matched the above dynamic template mapping and the second one matched the second dynamic template mapping.
            -   Mapping created is as follows
                {
                    "test_index": {
                        "mappings" : {
                            "properties" : {
                                "text_product_description" : {
                                    "type" : "text",
                                },
                                "text_product_id_keyword" : {
                                    "type" : "keyword"
                                }
                            }
                        }
                    }
                }
        -   For more flexibility the match parameter provides with wildcards, we can set a parameter named "match_pattern" to "regex".
        -   What it does is to adjust the behaviour of the "match" parameter to support regular expressions.
        -   PUT /test_index
            {
                "mappings" : {
                    "dynamic_templates" : [
                        {
                            "names" : {
                                "match_mapping_type" : "string",
                                "match" : "^[a-zA-Z]+_name$",
                                "match_pattern" : "regex",
                                "mapping" : {
                                    "type" : "text"
                                }
                            }
                        }
                    ]
                }
            }
        -   In this case the regular expression matches strings that end with "_name".
        -   Adding a document with these three field names will therefore create the following mapping.
            POST /test_index/doc
            {
                "first_name" : "John",
                "middle_name" : "Edward",
                "last_name" : "Doe"
            }
            {
                "properties" : {
                    "first_name" : {
                        "type" : "text",
                    },
                    "middle_name" : {
                        "type" : "text"
                    },
                    "middle_name" : {
                        "type" : "text"
                    }
                }
            }
    -   path_match and path_unmatch parameters
        -   These parameters evaluate the full field path
            -   i.e not just the field names
        -   This is a dot notation that you saw earlier
            -   E.g name.first_name
        -   wildcards are also supported.
        -   Eg: here another mapping will be created which is "full_name"
            -   PUT /test_index
                {
                    "mappings" : {
                        "dynamic_templates" : [
                            {
                                "copy_to_full_name" : {
                                    "match_mapping_type" : "string",
                                    "path_match" : "employer.name.*",
                                    "mapping" : {
                                        "type" : "text",
                                        "copy_to": "full_name"
                                    }
                                }
                            }
                        ]
                    }
                }
        -   Creating doc
            -   POST /test_index/_doc
                {
                    "employer" : {
                        "name" : {
                            "first_name" : "John",
                            "middle_name" : "Edward",
                            "last_name" : "doe"
                        }
                    }
                }
        -   Mapping created is as follows
            -   {
                    "employer" : {
                        "properties" : {
                            "name" : {
                                "properties" : {
                                    "first_name" : {
                                        "type" : "text",
                                        "copy_to" : ["full_name"]
                                    },
                                    "middle_name" : {
                                        "type" : "text",
                                        "copy_to" : ["full_name"]
                                    },
                                    "middle_name" : {
                                        "type" : "text",
                                        "copy_to" : ["full_name"]
                                    }
                                }
                            }
                        }
                    }
                }

        -   Within the "mapping" key of dynamic template, you can make use of two placeholders.
            -   The first one is named "dynamic_type"
            -   The "dynamic_type" placeholder is replaced with the data type that was detected by
                dynamic mapping.
            -   This template matches all datatypes and adds a mapping of that same data type.
            -   If this placeholder was not there then we had to define dynamic template for each datatype, which is quite
                inconvenient.
            -   The purpose of this template is to set index parameter to false. The datatype is going to be the same as it
                otherwise would be with dynamic mapping.
            -   This particular example with disabling indexing could be used for time series data.

            -   Ex:
                -   PUT /test_index
                {
                    "mappings" : {
                        "dynamic_templates" : [
                            {
                                "no_doc_values" : {
                                    "match_mapping_type" : "*",
                                    "mapping" : {
                                        "type" : "{dynamic_type}",
                                        "index" : false
                                    }
                                }
                            }
                        ]
                    }
                }

                -   POST /test_index/_doc
                    {
                        "name" : "John Doe",
                        "age" : 26
                    }
                -   Mapping created is as follows

                    {
                        "properties" : {
                            "age" : {
                                "type" : "long",
                                "index" : false
                            },
                            "name" : {
                                "type" : "text",
                                "index" : false
                            }
                        }
                    }

    -   Index templates vs dynamic templates
        -   Index templates apply mappings and index settings for matching indices
            -   This happens when indices are created and their names match a pattern
        -   Dynamic templates are evaluated when new fields are encountered
            -   and dynamic mapping is enabled.
            -   The specified field mapping is added the template's condition match
        -   Index templates define fixed mappings; dynamic templates are dynamic



    -   Stemming & Stop words
        -   Stemming reduces word to their root form. Why this is required?. suppose we index the document with field value as "loved".
            But we want to search with the value as love. In that case we won't be able to search the document.
            -   E.g loved -> love and drinking -> drink
            -   Stemming the given sentence "I loved drinking bottles of wine on last year's vacation" --> "I love drink bottl of wine last year vacat".
            -   When stemming is done it may create some invalid words depending on the level of stemming applied.

        -   Introduction to stop words
            -   Words that are filtered out during text analysis
                -   Common words such as "a", "the", "at", "of", "on", etc.
            -   They provide little to no value for relevance scoring
            -   Fairly common approach to remove such words.
                -   Less common in Elasticsearch today than in the past.
                    -   The relevance algorithm has been improved significantly.
            -   Not removed by default and generally not recommended to remove such words.
            -   Eg: removing stop words from the given sentence
                -   "I loved drinking bottles of wine on last year's vacation" --> "I loved drinking bottles wine last year's vacation"


    -   Analyzers and Search Queries
        -   Analyzers are not only used when indexing "text" fields but also used for search queries as well.
        -   One may ask how we are able to search for values that we index if they are changed during the analysis process,
            like using stemming and stop words
        -   Let's take an example
            -   "I loved drinking bottles of wine on last year's vacation"
            -   After analysing i.e converting to lower case and using stemming but skipping stop words.
            -   ["i", "love", "drink", "bottl", "of", "wine", "on", "last", "year", "vacat"]
            -   Now what will happen if we try to search for a given value
                GET /stemming_test/_search
                {
                    "query" : {
                        "match" : {
                            "description" : "drinking"
                        }
                    }
                }
            -   When indexing the word "drinking" was stemmed to its root form. So, when doing the search query, it will be analysed
                in the same way i.e "drinking" will be converted to word "drink"

            -   So when the document is indexed, Elasticsearch inspects the mapping for the "description" field. It detects that
                it's a "text" field, which means that it should be analyzed.
            -   It then checks if the analyzer is configured for the field. If so that analyzer is used and otherwise the "standard"
                analyzer is used.
            -   POST /stemming_test/_doc
                {
                    "description" : "I loved drinking bottles of wine on last year's vacation"
                }
            -   analyzer used
                {
                    "properties" : {
                        "description" : {
                            "type" : "text",
                            "analyzer" : "stemming_analyzer"
                        }
                    }
                }
            -   In this example a custom analyzer named "stemming_analyzer" is configured for the field, so that one is used
                in favor of the "standard" analyzer.
            -   It is similar to standard analyzer except the fact that the words of English language are stemmed.
            -   original values of the fields are present in the _source object. But searching is not done using _source field.

    -   Built in analyzers
        -   standard analyzer
            -   Splits text at word boundaries and removes punctuation.
                -   Done by the standard tokenizer
            -   Lowercases letter with lowercase token filter
            -   Contains the stop token filter (disabled by default) to remove stop words.
            -   Ex:
                -   "Is that Peter's cute-looking dog?"
                -   ["is", "that", "peter", "cute", "looking", "dog"]

        -   simple analyzer
            -   Similar to standard analyzer
                -   Splits into token when encountering anything else than letters
            -   Lowercases letter with lowercase tokenizer and not token filter unlike standard analyzer.
                -   unusual and a performance hack
            -   Ex:
                -   "Is that Peter's cute-looking dog?"
                -   ["is", "that", "peter", "s", "cute", "looking", "dog"]

        -   whitespace analyzer
            -   Splits text into tokens by whitespace
                -   Does not lowercase letters
            -   Ex:
                -   "Is that Peter's cute-looking dog?"
                -   ["Is", "that", "Peter's", "cute-looking", "dog?"]


        -   keyword analyzer
            -   No-op analyzer that leaves the input text intact
                -   It simply output it as a single token
            -   Used for keyword fields by default
                -   used for exact matching
            -   Ex:
                -   "Is that Peter's cute-looking dog?"
                -   ["Is that Peter's cute-looking dog?"]

        -   pattern analyzer
            -   A regular expression is used to match token separators
                -   It should match whatever should split text into tokens
            -   This analyzer is flexible
            -   The default pattern matches all non-word characters(\W+)
            -   lowercases letter by default.
            -   Ex:
                -   "Is that Peter's cute-looking dog?"
                -   ["is", "that", "peter", "s", "cute", "looking", "dog"]


        -   Last but not least, there are number of language specific analyzers. Can be found in the documentation.

        -   How to use them in field mappings?
            -   PUT /products
                {
                    "mappings" : {
                        "properties" : {
                            "description" : {
                                "type" : "text",
                                "analyzer" : "english"
                            }
                        }
                    }
                }
            -   POST /products/_doc
                {
                    "description" : "Is that Peter's cute-looking dog?"
                }
            -   After analysis
                ["peter", "cute", "look", "dog"]
            -   This analyzer is used both at index and search time.

        -   Configuring built-in analyzers
            -   While all of the built in analyzers can be used in this way, many of them can also be configured to adjust
                their behaviour.
            -   For example, the standard analyzer can be configured to remove stopwards, which it doesnt do by default.
            -   Instead of having to implement a custom analyzer from scratch, we can configure the built-in analyzer.
            -   In the below example we are creating a custom analyzer by extending the standard analyzer specified in the type parameter.
            -   Here we have named it as remove_english_stop_words which is just an arbitrary name.
            -   PUT /products
                {
                    "settings" : {
                        "analysis" : {
                            "analyzer" : {
                                "remove_english_stop_words" : {
                                    "type" : "standard",
                                    "stopwords" : "__english__"
                                }
                            }
                        }
                    }
                }
            -   For more information Built-in analyzer reference can be used for more parameters that can be passed for customization
            -   Now since we have created our own custom analyzer then how to use it. Just pass in the mapping
            -   Ex
                -   PUT /products/_mapping
                    {
                        "properties" : {
                            "description" : {
                                "type" : "text",
                                "analyzer" : "remove_english_stop_words"
                            }
                        }
                    }

    -   Creating Custom Analyzers
        -   Suppose, we have a given text to analyze
            "I&apos;m in a <em>good</em> mood&nbsp;-&nbsp;and I <strong>love</strong> acai!"
        -   Using the standard analyzer to analyze this text will yield strange results;
        -   POST /_analyze
            {
                "analyzer" : "standard",
                "text" : "I&apos;m in a <em>good</em> mood&nbsp;-&nbsp;and I <strong>love</strong> acai!"
            }
        -   After analyzing, the html text in the text will become terms, and so do the html tags, excluding the symbols.
        -   This is not ideal, because we don't want search queries to match any html.
        -   Fortunately there is a character filter that does two things; it strips out HTML tags and it converts HTML
            entities to their decoded values.
                -   POST /_analyze
                    {
                        "char_filter" : ["html_strip"]
                        "text" : "I&apos;m in a <em>good</em> mood&nbsp;-&nbsp;and I <strong>love</strong> acai!"
                    }
                -   Since we haven't specified a tokenizer, so the input text is just emitted as a single term.
                -   {
                        "tokens" : [
                            {
                                "token" : "I'm in a good mood - and I love acai!",
                                "start_offset" : 0,
                                "end_offset" : 78,
                                "type" : "word",
                                "position" : 0
                            }
                        ]
                    }
        -   To create a custom analyzer it should be define settings.analysis.analyzer object with its name.
        -   Now, in the previous example we configured a built-in analyzer, but here we are creating a custom analyzer so the type is set
            to custom. This will tell elasticsearch that we are creating our custom analyzer.
        -   Then we specify the name of the character filter within a parameter named "char_filter", so exactly what we just did with
            the analyze API.
        -   Adding the standard tokenizer.
        -   Adding token filters in the form of array.
            -   lowercase filter to lowercase the text fields
            -   stop filter to remove stop words
            -   asciifolding filter to convert the letters to their ascii equivalent. Here ascii equivalent does not mean converting it to
                number.
        -   PUT /analyzer_test
            {
                "settings" : {
                    "analysis" : {
                        "analyzer" : {
                            "my_custom_analyzer" : {
                                "type" : "custom",
                                "char_filter" : ["html_strip"],
                                "tokenizer" : "standard",
                                "filter" : [
                                    "lowercase",
                                    "stop",
                                    "asciifolding"
                                ]
                            }
                        }
                    }
                }
            }
        -   Now to test the analyzer we will define a query. Since my_custom_analyzer belongs to the "analyzer_test" index
            specifically, we need to specify the index name within the endpoint.
        -   POST /analyzer_test/analyze
            {
                "analyzer" : "my_custom_analyzer"
                "text" : "I&apos;m in a <em>good</em> mood&nbsp;-&nbsp;and I <strong>love</strong> acai!"
            }
        -   The result generated will have input being tokenized, and the terms are lower cased. Couple of words are missing, being stop words.
            Lastly, word acai having special characters has been converted to plain letters.


    -   Adding analyzers to existing indices
        -   In above we added custom analyzers during index creation time. Now this cannot be the case always.
        -   Adding custom analyzer to an already existing index can also be one of the scenario. It is tricky.
        -   We added one analyzer above in index analyzer_test. To add another analyzer we can use the Update index setting api as follows.
            -   PUT /analyzer_test/_settings
                {
                    "analysis" : {
                        "analyzer" : {
                            "my_second_analyzer" : {
                                "type" : "custom",
                                "char_filter" : ["html_strip"],
                                "tokenizer" : "standard",
                                "filter" : [
                                    "lowercase",
                                    "stop",
                                    "asciifolding"
                                ]
                            }
                        }
                    }
                }
        -   But when we try to add this we get error stating that we cannot update non-dynamic settings for open indices.
        -   Open and closed indices
            -   An open index is available for indexing and search requests
            -   A closed index will refuse requests
                -   Read and write requests are blocked.
        -   Dynamic and static settings
            -   In context of settings, there are two kinds of settings; static and dynamic.
            -   Dynamic settings can be changed without closing the index first i.e an index that is actively serving requests.
                -   Requires no downtime.
            -   Static settings require the index to be closed first.
                -   The index will be briefly unavailable.
            -   Analysis settings are static settings.
        -   Since we were changing settings related to analysis so we got the above exception.
        -   To add the above analyzer, first we need to close the index, add the custom analyzer and then open the index
            -   POST /analyzer_test/_close
            -   Now run the above put request to add the new custom analyzer
            -   POST /analyzer_test/_open
        -   To check if the analyzer is already added, we will check the settings.
            -   GET /analyzer_test/_settings
        -   This needs to be done in the same way, if we want to modify character filters, tokenizer, or token filters.

    -   Opening & closing indices
        -   Fairly quick, but not be an option for production clusters.
            -   E.g. mission critical systems where downtime is unacceptable.
        -   Alternatively reindex documents into a new index
            -   Create the new index with the updated settings
            -   use an index alias for the transition
    -   Updating analyzers










            
   





        
        


