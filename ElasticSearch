ElasticSearch
-	OpenSource Analytics and full Search Engine

Usages
	-	Query and analyze structured data
	-	Analyse application logs and system metrics
	-	Application performance management
	-	Send events to elasticsearch
	-	Forecast future values with machine learning
	-	Anomality detection

How does ElasticSearch work?
	-	Data is stored as documents. Similar to rows in SQL.
	-	A document's data is separated into fields. Similar to
		columns in relational databases.
	-	Written in Java, build on Apache lucene.
	-	Easy to use, and highly scalable.

ElasticSearch Stack
	-	Kibana
		-	An analytics and visualisation platform.
	-	Beats
		-	A collection of data shippers that send data to Elastisearch or Logstash.
	-	LogsStash
		-	A data processing pipeline
	-	X-Pack
		-	Adds additional features to Elasticsearch and kibana.

		

━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
✅ Elasticsearch security features have been automatically configured!
✅ Authentication is enabled and cluster connections are encrypted.

ℹ️  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
  LkcIgwxHsiO-ch7OfdL*

ℹ️  HTTP CA certificate SHA-256 fingerprint:
  6a0b4a461f8bfcec1efddfcb0561b1acd62004bd40bf87d61b2285c0078380b8

ℹ️  Configure Kibana to use this cluster:
• Run Kibana and click the configuration link in the terminal when Kibana starts.
• Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
  eyJ2ZXIiOiI4LjYuMiIsImFkciI6WyIxOTIuMTY4LjAuNTY6OTIwMCJdLCJmZ3IiOiI2YTBiNGE0NjFmOGJmY2VjMWVmZGRmY2IwNTYxYjFhY2Q2MjAwNGJkNDBiZjg3ZDYxYjIyODVjMDA3ODM4MGI4Iiwia2V5IjoiT0htNERJY0IzY21Xd1pKRmJpYm06U1JGTms3MWhTbU9keno4U1VLVHhaUSJ9

ℹ️  Configure other nodes to join this cluster:
• On this node:
  ⁃ Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`.
  ⁃ Uncomment the transport.host setting at the end of config/elasticsearch.yml.
  ⁃ Restart Elasticsearch.
• On other nodes:
  ⁃ Start Elasticsearch with `bin/elasticsearch --enrollment-token <token>`, using the enrollment token that you generated.


elastic password - LjqY3C+_53nh01LRrP0H

reset password - bin/elasticsearch-reset-password -u elastic
eyJ2ZXIiOiI4LjYuMiIsImFkciI6WyIxOTIuMTY4LjAuNTU6OTIwMCJdLCJmZ3IiOiJhZTc2ZTU3NzU0NzMzZGQ2ZDllYTY3OTA5MzQ2ZGE1ZTMwOTFhNDIyNTkwOGI0MTAwY2JkOWRkNzc4NDI3YTQ4Iiwia2V5IjoianpuUjlZWUJrOUVtUGNGZFNjbHo6bzVvS04yS2RUZGU5T3hMWUpybnFXQSJ9
kibana enrollment token - 
new enrolment token for kibana - bin/elasticsearch-create-enrollment-token --scope kibana



Basic Architecture Summary
	-	Nodes store the data that we add to elasticsearch.
	-	A cluster is a collection of nodes.
	-	Data is stored as documents, which are Json objects.
	-	Documents are grouped together with indices.


Sharding and Scalability
	-	Sharding is a way to divide indices into smaller pieces.
	-	Each piece is called as shard.
	-	Sharding is done at the index level and not cluster or node level.
	-	The main purpose is to horizontally scale the data volume.

Let's dive deeper
	-	A shard is an independent index... kind of.
	-	Each shard is an Apache lucene index.
	-	An ElasticSearch index consists of one or more Lucene indices.
	-	A shard has no predefined size; it grows as documents are
		added to it.
	-	A shard may store up to 2 billion documents.
The purpose of Sharding
	-	Mainly be able to store more documents
	-	To easier fit large indices onto nodes.
	-	Improved performance.
		-	Parallelization of queries increases the throughput of an index.

Configuring the number of shards
	-	An index contains single shard by default. See the pri field when running GET _cat/indices?v&expand_wildcards=all
	-	Also, while creating an index, it creates by default 1 primary shard and 1 replica shard.
		If elastic is running on single node only, then the replica shard is unassigned. In that case
		cluster health will be yellow instead of green.
	-	Indices in elasticSearch < 7.0.0 were created with five shards
		-	This often led to oversharding.
	-	Increase the number of shards using the Split API in new elasticSearch.
	-	Reduce the number of shards with the Shrink API.
	-	Once you set the number of shards for an index in ElasticSearch, you cannot change them. You will need to create a new index with the desired number of shards, and depending on your use case, you may want then to transfer the data to the new index

Introduction to Replication.
	-	What happens if a node's hard drive fails?
	-	Hardware can fail at any given time, so we need to handle that somehow
	-	ElasticSearch supports fault tolerance using replication.
	-	Replication is supported natively and enabled by default.


How does replication work?
	-	Replication is configured at index level.
	-	Replication works by creating copies of shards, referred to as replica shards.
	-	A shard that has been replicated, is called primary shard.
	-	A primary shard and its replica shards are referred to as 
		a replication group.
	-	Replica shards are complete copy of a shard.
	-	A replica shard can serve search requests, exactly like its primary shard.
	-	The number of replicas can be configured at index creation.
	-	Example :
		-	Primary Shard A, Replica A1, Replica A2 <- Replication Group A
		-	Primary Shard B, Replica B1, Replica B2 <- Replication Group B
		- The above two collectively form an index.
	- But what if entire disk stops working and we loose all of the data?
	- To avoid this replica shards are not created on the same node of their primary shard.
	- For example Node A will contain primary Shard A, replica shard B1 and replica shard B2.
	And Node B will contain primary Shard B, replica shard A1 and replica shard A2.

Snapshots
	-	Elasticsearch supports taking snapshots as backups.
	-	Snapshots can be used to restore to a given point in time.
	-	Snapshots can be taken at the index level, or for the entire cluster.
	-	Use snapshots as backups and replication for high availability(and performance).
	-	Replications work on live data, but what if live data gets corrupted. In that case we need to rollback in which
	snapshots provides the backups.
	-	Replications also helps in increasing the throughput.
		If there are multiple requests then elasticsearch will delegate the requests to the shards.


Adding more nodes to cluster
	-	Sharding enables us to scale an index data volume
	-	But eventually we will need to add additional nodes
	-	Also, replication requires at least two nodes.
	-	We will try to add two more nodes to our cluster. This can be done in local setup only.
	-	To make nodes first try to extract the elastic archive. If you want to create two nodes then
		make two copies. Make sure not to copy the existing node's directory. Because it contains data used by that node.
	-	For each node extract the archive and create.
	-	Now rename each node by going to /config/applcation.yml.
	-	First time whenever we start a node we need to specify the enrollment token. Using enrollment token helps elasticsearch to use the configurations that we don't need to do.
	-	First go to the existing node folder and create the enrollment token using
		bin/elasticsearch-create-enrollment-token -s node
	-	copy the enrollment token and then start the other nodes using this command
		bin/elasticsearch --enrollment-token <enrollment-token>
	Word of Caution
		-	After adding third node, atlease two nodes are required to run the cluster
			-	If two nodes are lost, the cluster cannot elect a master node.
		-	If you want to run a single node moving forward, don't add the third node.
			-	Alternatively you can start fresh with a single-node cluster afterwards.


Overview of Node roles
	-	Master-eligible
		-	The node may be elected as the cluster's master node.
		-	A master node is responsible for creating and deleting indices, among others.
		-	A node with this role will not automatically become the master node.
			-	unless there are no other master-eligible nodes
		-	May be used for having dedicated master nodes. Because Master node should not be busy in doing other things, such as serving requests. Also, if master node is doing other things then there may be high cpu consumption, memory consumption.
			-	Useful for large clusters

	-	Data Node
		-	Enables a node to store data.
		-	Storing data includes performing queries related to that data, such as search queries
		-	For relatively small clusters, this role is almost always enabled.
		-	Useful for having dedicated master master nodes.
		-	Used as part of configuring a dedicated master node.

	-	Ingest Nodes
		-	Enables a node to run ingest pipelines.
		-	Ingest pipelines are a series of steps(processors) that are performed when indexing documents.
			-	Processors may manipulate documents, eg. resolving an IP to lat/lon
		-	A simplified version of Logstash, directly within Elasticsearch.
		-	This role is mainly useful for having dedicated ingest nodes.

	-	Machine Learning Node
		-	node.ml identifies a node as a machine learning node.
			-	This lets the node run machine learning jobs
		-	xpack.ml.enabled enables or disables the machine learning API for the node.
		-	Useful for running ML jobs that don't affect other tasks.

	-	Coordination Node
		-	Coordination refers to the distribution of queries and the aggregation of results.
		-	Useful for coordination-only nodes(for large clusters)
		-	Configured by disabling all other roles.

	-	Voting Node
		-	Rarely used, and you almost certainly won't use it either.
		-	A node with this role, will participate in the voting for a new master node.
		-	The node cannot be elected as the master node itself, though
		-	Only used for large clusters.


Creating and Deleting Indices
	-	DELETE /pages
	-	PUT /products
	-	PUT /products
		{
			"settings" : {
				"number_of_shards" : 2,
				"number_of_replicas" : 2
			}
		}
	-	Here there will be only 1 primary shard and the other two replcia shards will be identical to
		its primary shard. Thereby forming a replication group. so when data is inserted it will be inserted in the 3 shards.
	-	Modifying new indices requires reindexing docs to new index.	

Inserting Docs
	-	POST /products/_doc
		{
			"name" : "Coffee Maker",
			"price": 64,
			"in_stock": 10
		}
		-	This will insert a document with random generated index and will be copied to the 3 shards.
	-	PUT /products/_doc/100
		{
			"name" : "Toaster",
			"price": 55,
			"in_stock": 4
		}
		-	Since we have inserted the first doc, now to add more docs we will use PUT instead of POST.
		-	Also here we are specifying the id value as 100.
		-	POST is ususally used for automatic id creation wheras for PUT add new doc by providing an id.

Retrieving Docs
	-	GET /products/_doc/100
		-	Here if the document exists then, found flag will be true and the doc will show under _source flag.
		-	Otherwise, found flag will be false and _source flag will not be present.


Updating Docs
	-	POST /products/_update/100
		{
			"doc" : {
				"in_stock" : 3
			}
		}
	-	Adding fields to the same doc can be done using the same above query
	-	POST /products/_update/100
		{
			"doc" : {
				"tags" : ["electronics"]
			}
		}

Documents are immutable
	-	Elasticsearch documents are immutable (!)
	-	We actually replaced documents
	-	The update API did some things for us, making it look like we updated documents
	-	The Update API is simpler and saves some network traffic.

How the update API works
	-	The current document is retrieved.
	-	The field values are changed.
	-	The existing document is replaced with modified document.
	-	We could do the exact same thing at application level.


Scripted Updates
	-	POST /products/_update/100
		{
			"script" : {
				"source" : "ctx._source.in_stock--"
			}
		}
		-	subracts the instock value by 1.
	-	POST /products/_update/100
		{
			"scripts" : {
				"source" : "ctx._source.in_stock = 10"
			}
		}

	-	POST /products/_update/100
		{
			"script" : {
				"source" : "ctx._source.in_stock -= params.quantity",
				"params" : {
					"quantity" : 4
				}
			}
		}

	-	POST /products/_update/100
		{
			"script" : {
				"source": """
					if (ctx._source.in_stock > 0) {
						ctx._source.in_stock--;
					}
				"""
			}
		}


Upserting Documents
	-	If the document does not already exist, the contents of the upsert element are inserted as a new document. If the document exists, the script is executed.
	-	POST /products/_update/101
		{
			"script" : {
				"source" : "ctx._source.in_stock++"
			},
			"upsert" : {
				"name" : "Blender",
				"price" : 399,
				"in_stock" : 5
			}
		}
	-	If the document is not present then the result keyword will have created as value, otherwise it will have updated as value


Replacing Documents
	-	PUT /products/_doc/101
		{
			"name" : "Toaster",
			"price": 79,
			"in_stock": 4
		}


Deleting Documents
	-	DELETE /products/_doc/101


Introduction to Routing
	-	How does ElasticSearch know where to store documents ?
	-	How are documents found once they have been indexed ?
	-	The answer is routing 
	-	Routing is the process of resolving a shard for a document.
	-	It is resolved by using the formula.
		-	shard_num = hash(_routing) % num_primary_shards.
	-	Same formula is used when retrieving/updating/deleting the document.
	-	When you index a document, Elasticsearch will determine which shard the document should be routed to for indexing. Where the default value of _routing is _id.
	-	When using custom routing, in that case _routing value is different.



Custom Routing
	-	Routing is 100% transparent when using ElasticSearch.
	-	This makes Elasticsearch easier to use and learn.
	-	It is possible to customize routing, though, for various purposes.
	-	The default routing strategy ensures that the documents are distributed evenly.

Intresting fact
	-	As we know we cannot change the number of shards after the index has been created. This is because
		routing formula depends on the number of shards.


How ElasticSearch Reads data
	-	A read request is recieved and handled by coordinating node.
	-	Routing is used to resolve the document's replication group.
	-	ARS is used to send the query to the best available shard.
		-	ARS is short for Adaptive Replica Selection.
		-	ARS helps reduce query response time.
		-	ARS is essentially an intelligent load balancer.
	-	The coordinating node collects the response and sends it to the client.


How ElasticSearch Writes Data
	-	Write operations are sent to primary shards.
	-	The primary shard forwards the operation to replica shards.
	-	Primary terms and sequence numbers are used to recover from failures.
	-	Primary term
		-	Counter denoting how many times a Primary shard has been elected.
		-	Initially the counter is set to 1, when a failure occurs and a new primary shard
			has been elected. In that case, counter is increment to 2.
	-	Sequence Number
		-	Counter for each write operation and is only incremented by Primary shard.
	-	Global and local checkpoints help speed up the recovery process.
	-	Global Checkpoint
		-	Checkpoint for every replication group. Basically minimum sequence number that is common in replication group.
	-	Local CheckPoint
		-	Checkpoint for every replica shard. Basically maximum sequence number out of all the replica shards.
	-	Primary term and sequence numbers are available within responses.

Introduction to Versioning
	-	Not a revision history of documents.
	-	Elasticsearch stores an _version metadata field with every document.
		-	The value is an integer
		-	It is incremented by one when modifying a document.
		-	The value is retained for 60 seconds when deleting a document.
			-	Configured with the index.gc_deletes setting.
		-	The _version field is returned when retrieving documents.

	-	Types of Versioning
		-	The default versioning type is called internal versioning.
		-	There is also an external versioning type 
			-	Useful when versions are maintained outside of ElasticSearch.
			-	Eg: When documents are also stored in RDBMS

What is the point of Versioning?
	-	You can tell how many times document has been modified.
		-	Probably not that useful

	-	Versioning is hardly used anymore, and is mostly a thing from the past.
	-	It was previously the way to do optimistic concurrency control.
		-	Now there is better way using primary terms and sequence numbers.
	-	This _version field is being used for clusters running old versions.


Optimistic Concurrency Control
	-	Prevent overwriting documents inadvertently due to concurrent operations.
	-	There are many scenarios in which this can happen
		-	handling concurrent visitors for web application.


How do we handle failures?
	-	Handle the situation at application level.
		-	Retreive the document again.
		-	Use _primary_term and _seq_no for new update request.
		-	Remember to perform any calculations that use field values again.


Summary Concurrency Control
	-	Sending write requests to ElasticSearch concurrently may overwrite changes
		made by other concurrent processes.
	-	Traditionally, the _version field was used to prevent this
	-	Today, we use the _primary_term and _seq_no fields.
	-	Elasticsearch will reject a write operation if it contains the wrong primary term or sequence number
		-	This should be handled at application level.



Update by query
	-	Problem - To decrease the in_stock field by one for every products.
	-	POST /products/update_by_query
		{
			"script" : {
				"source" : "ctx._source.in_stock--"
			}
			"query" : {
				"match_all" : {}
			}
		}
	-	This will update the stock for all products. Here we are specifying constraints using
		query keyword.

	-	If a document has been modified since taking the snapshot, the query is aborted.
		-	This is checked using document's primary term and sequence number.
	-	To count version conflicts instead of aborting the the query, the conflicts option
		can be set to proceed.


Delete by query
	-	To delete all the documents
	-	POST /products/delete_by_query
		{
			"query" : {
				"match_all" : {

				}
			}
		}


Batch Processing
	-	POST /_bulk
		{ "index" : { "_index" : "products", "_id" : 200}}
		{"name" : "Espresso Machine", "price" : 199, "in_stock" : 5}
		{"create" : {"_index" : "products", "_id" : 201}}
		{"name" : "Milk Frother", "price" : 149, "in_stock" : 14}

	-	here index and create is a type of action that we want to perform. Diff is that create action will give an error if the document already exists, but index will replace the document if it already exists. If doc is not present in that case it will be inserted for both the action type
	-	2nd and 4th line describes the doc information that we want to store.

	-	POST /_bulk
		{ "update" : { "_index" : "products", "_id" : 200}}
		{ "doc" : {"price" : 129}}
		{ "delete" : { "_index" : "products", "_id" : 200}}

	-	Or you can use this if all the operations are on same index
		POST /products/_bulk
		{ "update" : {"_id" : 200}}
		{ "doc" : {"price" : 129}}
		{ "delete" : {"_id" : 200}}



Analysis
	-	Sometimes referred to as text analysis
	-	Applicable to text field/values.
	-	Text values are analyzed when indexing documents.
	-	The result is stored in data structures that are efficient for searching etc.
	-	The _source object is not used when searching for documents
		-	It contains the exact values specified when indexing documents.
	-	When a text value is indexed, a so-called analyzer is used to proces the text.
	-	An analyzer contains of three building blocks;
		-	character filters
		-	tokenizer
		-	token filters
    -   The result of analyzing text values is then stored in a searchable data structure.
    
    -   Character Filters
        -   Adds, removes or changes characters.
        -   Analyzer contains zero or more character filters.
        -   Character filters are applied in the order in which they are specified.
        -   Example (html_strip) filter
            -   Input: "I&apos;m in a <em>good</em> mood&nbsp;-&nbsp; and I<strong>love</strong> acai"
            -   Output: "I am in a good mood and I love acai"   

    -   Tokenizer
        -   An analyzer contains one tokenizer
        -   Tokenizes a string, i.e splits it into tokens.
        -   Characters may be stripped as a part of the tokenization.
        -   Example: 
            Input : "I REALLY like beer!"
            Output : ["I", "REALLY", "like", "beer"]
        -   Although not seen here, tokenizer also records the character offsets in the original string.

    -   Token Filters
        -   Receive the output of the tokenizer as input(i.e the tokens)
        -   A token filter can add, remove, or modify tokens
        -   An analyzer contains zero or more token filters.
        -   Token filters are applied in the order they are specified.
        -   Example(lowercase filter)
            -   Input: ["I", "REALLY", "like", "beer"]
            -   Output: ["i", "really", "like", "beer"]

    -   These three building blocks collectively make a standard analyser.
    -   The analyser is used for all text fields unless configured otherwise.
    -	Specifying the standard analyzer explicitly in the request.
    -   POST /_analyze/
		{
  			"text": "2 guys walk into  a bar, but the third... DUCKS! :-)",
  			"analyzer": "standard"
		}
	-	Response
		{
          "tokens": [
            {
              "token": "2",
              "start_offset": 0,
              "end_offset": 1,
              "type": "<NUM>",
              "position": 0
            },
            {
              "token": "guys",
              "start_offset": 2,
              "end_offset": 6,
              "type": "<ALPHANUM>",
              "position": 1
            },
            {
              "token": "walk",
              "start_offset": 7,
              "end_offset": 11,
              "type": "<ALPHANUM>",
              "position": 2
            },
            {
              "token": "into",
              "start_offset": 12,
              "end_offset": 16,
              "type": "<ALPHANUM>",
              "position": 3
            },
            {
              "token": "a",
              "start_offset": 18,
              "end_offset": 19,
              "type": "<ALPHANUM>",
              "position": 4
            },
            {
              "token": "bar",
              "start_offset": 20,
              "end_offset": 23,
              "type": "<ALPHANUM>",
              "position": 5
            },
            {
              "token": "but",
              "start_offset": 25,
              "end_offset": 28,
              "type": "<ALPHANUM>",
              "position": 6
            },
            {
              "token": "the",
              "start_offset": 29,
              "end_offset": 32,
              "type": "<ALPHANUM>",
              "position": 7
            },
            {
              "token": "third",
              "start_offset": 33,
              "end_offset": 38,
              "type": "<ALPHANUM>",
              "position": 8
            },
            {
              "token": "ducks",
              "start_offset": 42,
              "end_offset": 47,
              "type": "<ALPHANUM>",
              "position": 9
            }
          ]
        }

    -   In above you can see elastic is smart enough to remove commas, exclaimation marks and whitespaces also. This is done because they provide no value
        when performing text searches.
    -   Instead of specifying the analyzer we can also specify the parts of the analyzer like this but the result will be the same as mentioned above.
    -   POST /_analyze/
        {
          "text": "2 guys walk into  a bar, but the third... DUCKS! :-)",
          "char_filter": [],
          "tokenizer": "standard",
          "filter": ["lowercase"]
        }

UnderStanding Inverted Indices
    -   A field's value is stored in several data structures.
        -   The data structure depends on the field's dataType.
    -   Ensure efficient data access    -   e.g searches.
    -   Handled by Apache Lucene, not ElasticSearch.
    -   One such data structure is Inverted Index.

    -   Inverted Indices
        -   Mapping between terms and which documents contain them
        -   terms/tokens here are the output of the tokenizer.
        -   terms are sorted alphabetically
        -   Inverted indices contain more than just terms and document IDs
            -   Example of inverted index.
            -   Text field in Document 1 - "2 guys walk into a bar, but the third... DUCKS! :-)"
            -   Text field in Document 2 - "2 guys went into a bar"
            -   Text field in Document 3 - "2 ducks walk around the lake"
            -   Now for each of the text, it will be analyzed by the standard analyser and will be converted into tokens.
            -   Now a table will be created for the sorted tokens(observed in the documents) and the documents.
            -   Each entry in the table will show whether this token is present in a given document or not.

            -   This is an example showing that how inverted indices are create for a single text field present in the document.
            -   What happens if there are multiple text fields in the document? In that case inverted index is created for each text field.
            -   So, if a document has two text field then 2 inverted indices are created.
        -   One inverted index per text field
        -   Other data types use BKD trees as data structure.

Introduction to Mapping
    -   What is mapping?
        -   Defines the structure of documents and how they are stored. Eg. field and their datatypes
        -   Similar to table schema in a relational databse.
        -   Types of Mapping
            -   Explicit Mapping    -   We define field mappings ourselves.
            -   Dynamic Mapping     -   ElasticSearch will generate field mapping automatically.   


Overview of DataTypes
    -   Some of the basic ElasticSearch data types.
    -   object, integer, long, boolean, double, text, short, float and date.
    -   There are some specialised data types such as ip data types to store ip addresses.
    
    -   Object datatype
        -   Used for any JSON object
        -   Objects may be nested
        -   Mapped using properties parameter
        -   {
                "name: : "Coffee maker",
                "price" :  64.2,
                "in_stock" : 10,
                "is_active" : true,
                "manufacturer" : {
                    "name" : "Nespresso",
                    "country" : "Switzerland"
                }
            }

        -   PUT /products
            {
                "mappings" : {
                    "properties" : {
                        "name" : {"type" : "text"},
                        "price" : {"type" : "double"},
                        "in_stock": {"type" : "short"},
                        "is_active" : {"type" : "boolean"},
                        "manufacturer" : {
                            "properties" : {
                                "name" : {"type" : "text"},
                                "country" : {"type" : "text"}
                            }
                        }
                    }
                }
            }

        -   Objects are not stored as objects in Apache lucene.
            -   Objects are transformed to ensure that we can index any valid json.
            -   In particular, objects are flattened.
        -   The above example will be flattened as below
            -   {
                    "name" : "Coffee maker",
                    "price" : 64.2,
                    "in_stock" : 10,
                    "is_active" : true,
                    "manufacturer.name" : "Nespresso",
                    "manufacturer.country" : "Switzerland"
                }

        -   Now what if there are array of fields. In that case it will create something like this.
            -   {
                    "manufacturer.name" : ["Nespresso", "Nescafe"],
                    "manufacturer.country" : ["Switzerland", "India"]
                }
            -   It becomes difficult, to search documents like this. When running the below query, elastic won't be able to diffrentiate
                which manufacturer country belongs to which manufacturer name
            -   Query : Match products where manufacturere.name == "Nescafe" AND manufacturing.country == "India".


    -   nested data type
    	-   Similar to the object data type, but maintains objects relationship.
        -   Useful when indexing array of objects.
        -   Enables us to query objects independently. meaning object values are not mixed together as in the above example of array objects.
            -   Must use the nested query.
            -   {
                    "name" : "Coffee maker",
                    "reviews" : [
                        {
                            "rating" : 5.0,
                            "author" : "Average Joe",
                            "description" : "Haven't slept for days... Amazing!"
                        },
                        {
                            "rating" : 3.5,
                            "author" : "John Doe",
                            "description" : "Could be better"
                        }
                    ]
                }

            -   PUT /products
                {
                    "mappings" : {
                        "properties" : {
                            "name" : {"type" : "text"},
                            "reviews": {"type" : "nested"}
                        }
                    }
                }
            -   Query : Match products where reviews.author == "John Doe" AND reviews.rating >= 4.0
            -   This query now works correctly because the objects are stored independently and the query works correctly.
        -   nested objects are stored as hidden documents.
        -   Suppose that we index a document containing product info. and that document has a reviews array field which have ten review objects inside it.
            This would cause to index 11 documents into lucene, one for the product and 10 for the reviews.


    -   Keyword datatype
        -   Used for exact matching of values.
        -   Typically used for filtering, aggregations and sorting.
        -   E.g. searching for articles with the status of "PUBLISHED".
        -   for full text searches, use the text datatype instead. full text searches are the searches that do not require exact matches.
            -   Eg. searching the body text of an article 

    -	How the keyword datatype works
    	-	How keyword fields are analyzed
    		-	keyword fields are analyzed with keyword analyzer
    		-	The keyword analyzer is a no-op analyzer
    			-	It outputs the unmodified string as a single token.
                -   This token is then placed into the inverted index.
    			-	POST /_analyze
    				{
    					"text" : "2 guys walk into a bar, but the third... DUCKS! :-)",
    					"analyzer" : "keyword"
    				}
    			-	Response
    				{
                      "tokens": [
                        {
                          "token": "2 guys walk into a bar, but the third... DUCKS! :-)",
                          "start_offset": 0,
                          "end_offset": 51,
                          "type": "word",
                          "position": 0
                        }
                      ]
                    }

                -   As you can see the string is untouched and returned as it is as a single token. If symbols were to be stripped out, for instance, 
                    we wouldn't be able to perform exact searches.
                -   Why, because the values within the inverted index (in the form of term/tokens) would then differ from the string that was provided with the indexed document.
                    Searching for the original value would therefore yield no results.
                -   what it would look like if we if save the full text as a token/term.
                    TERM                                    Document1   Document2   Document3
                    2 ducks walk around the lake            X
                    2 guys walk into a bar, but                         X
                    the third... DUCKS! :-)                     
                    2 guys went into a bar.                                         X

                -   This is unlike the text fields where the standard analyzer is used.
                -   Also keyword analayzer does not removes symbols and lower case letters.
                -   Much more realistic example when keyword analyzer is used is to search emailAddresses
            -   keyword fields are used for exact matching, aggregations and sorting.

Introduction to type coercion
	-	Data types are inspected when indexing documents.
		-	They are validated, and some invalid values are rejected.
		-	E.g trying to index an object for a text field.
	-	Sometimes, providing the wrong data type is okay
	-	Eg.-
		-	PUT /coercion_test/_doc/1
			{
				"price" : 7.4
			}
		-	PUT /coercion_test/doc/2
			{
				"price": "7.4"
			}
		-	PUT /coercion_test/doc/3
			{
				"price" : "7.4m"
			}
		-	GET /coercion_test/_doc/2
		-	coercion_test index is yet not created. It will be created when the first document is inserted.
		-	When the document is inserted then a dynamic mapping of datatype is created for the price field. In this case it will be float.
		-	What ElasticSearch did was to inspect the value that we supplied in order to figure out its datatype. In this case we supplied a floating point number, so the float data type was used within the mapping.
		-	In our second doc a floating point number is represented but in the form of string. If we run the query then it will also indexed. We can see that for the same price field in first doc we are passing a float value and in another float value as a string.
		-	This is where type coercion comes into the picture.
		-	Elasticsearch first resolves the data type that was supplied for a given field. In case there is a mapping then the two data types are compared.
		-	Since we supplied a string for a numeric data type, ElasticSearch inspects the string value.
		-	If it contains a numeric value and only numeric value Elasticsearch will convert the string to
			the correct data type, being float in this example.
		-	The result is that a string of 7.4 is converted into the floating point number 7.4 instead, and the document is indexed as if we had supplied a number in the first place.
		-	Now, if we try to pass a string that cannot be converted to float like the third document above, in that case it will give exception that it cannot be converted.
		-	Also if you try to get the second document it will return string "7.4" and not float 7.4 .
			But, as mentioned above that ElasticSearch coerces the value into a float instead of a string. So, why this behaviour ?
		-	The reason is that the "_source" key contains the original values that we indexed.
		-	UnderStanding the _source object
			-	Contains the values that are supplied at index time in our case it is("7.4")
				-	Not the values that are indexed(converted to 7.4 via coercion)
			-	Search queries use indexed values, not _source values.
				-	BKD trees, inverted indices etc.
				-	Elasticsearch searches the data structure used by Apache Lucene to store the field values.
				-	In case of text fields, that would be an inverted index.
				-	In this example, the value has been converted into a float, even though we see a string.
				-	Within apache lucene the value is stored as a numeric value and not a string.
			-	_source does not reflect how values are indexed
				-	Keep coercion in mind if you use values from _source.
		-	Coercion is not used for dynamic mapping.
			-	Supplying "7.4" for a new field will create a text mapping.
		-	Always try to use the correct datatype.
			-	Especially the first time you index a field.
		-	Disabling coercion is a matter of preference.
			-	It is enabled by default.



Introduction to arrays
	-	There is no such thing as array data type.
	-	Any field may contain zero or more values.
		-	No configuration or mapping needed.
		-	Simply supply an array when indexing document.
		-	POST /products/_doc
			{
				"tags" : "Smartphone"
			}
			POST /products/_doc
			{
				"tags" : ["Smartphone", "Electronics"]
			}
		-	The above two queries are valid and can be run in any order.
		-	The mapping that will be created is as follows
			{
				"products" : {
					"mapping" : {
						"properties" : {
							"tags" : {
								"type" : "text"
							}
						}
					}
				}
			}
		-	As we can see there is no sign of array data type in the mapping.
		-	So how does this get stored ?
		-	In the case of text fields, the strings are simply concatenated before being analyzed and
			the resulting token is stored within the inverted index as normal.
		-	POST /_analyze
			{
				"text" : ["Strings are simply", "merged together"],
				"analyzer" : "standard"
			}
			POST /_analyze
			{
				"text" : ["Strings",  "are", "simply", "merged",  "together"],
				"analyzer" : "standard"
			}
		-	If your run the above two analyze api then you would see that it is returning the same result.
			It means that the text values in array are stored as a same string.
		-	When storing the strings are concatenated with a space in between, because otherwise
			the word simply and merged would end up within the same token.
		-	In case of non-text fields the values are not anlayzed, multiple values are just stored within
			the appropriate data structure within Apache lucene.


		-	Constraints
			-	Array values should be of the same datatype. This means you cannot mix strings and integers.
			-	You can mix datatypes together only if passed datatype can be coerced with the mapping datatype.
			-	This is true for simple datatypes but not for the other ones such as objects.
				[{"name" : "Coffee Maker"}, {"name" : "Toaster"}, false]
				-	here false cannot be converted to object type.
			-	Coercion only works for fields that are already mapped.
				-	If creating a field mapping with dynamic mapping, an array must contain the same data type. Mixed data types are not allowed until the mapping is present. Mixed data type array can be passed after the mapping is created.

		-	Nested arrays
			-	Arrays may contain nested arrays
			-	Arrays are flattened during indexing.
			-	[1, [2, 3]] becomes [1, 2, 3].

Adding Explicit Mappings
	-	PUT /reviews
		{
			"mappings" : {
				"properties" : {
					"rating" : {
						"type" : "float"
					},
					"content" : {
						"type" : "text"
					},
					"product_id" : {
						"type" : "integer"
					},
					"author" : {
						"properties" : {
							"first_name" : {
								"type" : "text"
							},
							"last_name" : {
								"type" : "text"
							},
							"email" : {
								"type" : "keyword"
							}
						}
					}
				}
			}
		}
	-	Now adding documents
		PUT /reviews/_doc/1
		{
			"rating" : 5.0,
			"content" : "Outstanding course! Bo really taught me a lot about ElasticSearch!",
			"product_id" : 123,
			"author" : {
				"first_name" : "John",
				"last_name" : "Doe",
				"email" : "johndoe123@example.com"
			}
		}

	-	This above document will get inserted using the respective mappings.
	-	But if we change the value of email field from string to an {} denoting an object. Then elasticSearch won't be able to coerce the value.
	-	Also, instead of string if we change the field value of email to integer then elasticSearch will be able to coerce the value.

Retrieving Mappings
	-	To retrieve a value of given index. Just query the mapping api
	-	GET /reviews/_mapping
	-	To retrieve the mapping of a particular field, 
	-	GET /reviews/_mapping/field/content  where content is the field name.
	-	GET /reviews/_mapping/field/author.email where author.email is nested field.
                    

Using dot notations in field names
	-	the above explicit mapping of author object can be done in a simpler way
		-	PUT /reviews
		{
			"mappings" : {
				"properties" : {
					"rating" : {
						"type" : "float"
					},
					"content" : {
						"type" : "text"
					},
					"product_id" : {
						"type" : "integer"
					},
					"author.firstname" : {
						"type" : "text"
					},
					"author.lastName" : {
						"type" : "text"
					},
					"author.email" : {
						"type" : "keyword"
					}
				}
			}
		}

Adding mappings to existing indices
	-	To add new mappings to existing index, we will be using the mapping api.
	-	Since we are using the mapping api we don't need to add "mapping" keyword in the request body
	-	Instead just use the properties to define mapping for a new field.
	-	PUT /reviews/_mapping
		{
			"properties" : {
				"created_at" : {
					"type" : "date"
				}
			}
		}
	-	GET /reviews/_mapping

Introduction to Dates
	-	Specified in one of three ways.
		-	Specially formatted strings.
		-	Milliseconds since the epoch (long)
		-	Seconds since the epoch(integer)
	-	Epoch refers to 1st of January of 1970
	-	Custom date formats are supported.

	-	Default Behaviour of Date fields
		-	Three supported formats:
			-	A date without time
			-	A date with time
			-	Milliseconds since the epoch (long)

		-	In case of string timesamps UTC timezone is assumed if nothing is specified.
		-	If we supply a string value for the date value, the date needs to be in ISO 8601 specification
	-	How date fields are stored
		-	Stored internally as milliseconds since the epoch (long)
		-	Any valid value that you supply at the index time is converted to a long value internally.
		-	Dates are converted to UTC timezone
		-	The same date conversion also happen for date queries, too. This date will be converted to long before the search process begins.
	-	Example
		when only date is specified for date field. Elastic Search assumes time to be midnight of that particular day and will convert them to milliseconds since the epoch.
		PUT /reviews/_doc/2
		{
			"rating" : 4.5,
			"content" : "Not bad. Not bad at all",
			"product_id" : 123,
			"created_at" : "2015-03-27",
			"author" : {
				"first_name" : "Average",
				"last_name" : "Joe",
				"email" : "avgjoe@example.com"
			}
		}


		when date with time is specified for date field. Here the date is specified in utc timezone
		using the z.
		PUT /reviews/_doc/3
		{
			"rating" : 4.5,
			"content" : "Not bad. Not bad at all",
			"product_id" : 123,
			"created_at" : "2015-03-27T13:07:41Z",
			"author" : {
				"first_name" : "Average",
				"last_name" : "Joe",
				"email" : "avgjoe@example.com"
			}
		}

		when date with time is specified for date field.
		PUT /reviews/_doc/4
		{
			"rating" : 4.5,
			"content" : "Not bad. Not bad at all",
			"product_id" : 123,
			"created_at" : "2015-03-28T09:21:51+01:00",
			"author" : {
				"first_name" : "Average",
				"last_name" : "Joe",
				"email" : "avgjoe@example.com"
			}
		}

		when date with time is specified but in epoch. Here the epoch should be in milliseconds since epoch
		PUT /reviews/_doc/5
		{
			"rating" : 4.5,
			"content" : "Not bad. Not bad at all",
			"product_id" : 123,
			"created_at" : "1436011284000",
			"author" : {
				"first_name" : "Average",
				"last_name" : "Joe",
				"email" : "avgjoe@example.com"
			}
		}


		To get all the queries
		The values are the exact values that we specified when indexing the documents, and they do not
		represent how dates are stored for searching
		GET /reviews/_search
		{
			"query" : {
				"match_all" : {}
			}
		}

Handling Missing Fields.
	-	what happens when we don't provide field values during indexing a document.
	-	Fields can have zero or more values by default it means all fields are optional.
	-	You can leave a field out when indexing documents.
	-	E.g unlike relational databases where you need to allow null values.
	-	Some integrity checks need to be done at the application level
		-	E.g having required fields.
	-	Adding a field mapping does not make a field required. You can leave the field value optional with mapping present.
	-	Searches automatically handle missing fields. In other words, you don't need to actively handle
		if some documents may have left fields out.


Overview of Mapping Paramters
	-	format parameter
		-	Used to customize the format for the date fields
		-	Default date format is is recommended for use.
			-	"strict_date_optional_time || epoch_millis"
		-	Using Java's DateFormatter syntax
			-	Eg "dd/MM/yyyy"
		-	Using built-in formats
			-	E.g "epoch_seconds"
		-	Eg:
			PUT /sales
			{
				"mappings" : {
					"properties" : {
						"purchased_at" : {
							"type" : "date",
							"format" : "dd/MM/yyyy"
						}
					}
				}
			} 


			PUT /sales
			{
				"mappings" : {
					"properties" : {
						"purchased_at" : {
							"type" : "date",
							"format" : "epoch_second"
						}
					}
				}
			}
	-	properties parameter
		-	Defines nested fields for object and nested fields.
		-	Examples
			PUT /sales
			{
				"mappings" : {
					"properties" : {
						"sold_by" : {
							"properties" : {
								"name" : {
									"type" : "text"
								}
							}
						}
					}
				}
			}

	-	coerce parameter
		-	Used to enable or disable coercion values (enabled by default)
		-	PUT /sales
			{
				"mappings" : {
					"properties" : {
						"amount" : {
							"type" : "float",
							"coerce" : false
						}
					}
				}
			}
		-	It is possible to enable and disable coercion at index level. So you don't have to specify
			coercion parameter for every parameter/field.
			PUT /sales
			{
				"settings" : {
					"index.mapping.coerce" : false
				},
				"mappings" : {
					"properties" : {
						"amount" : {
							"type" : "float",
							"coerce": true
						}
					}
				}
			}
			-	Here coerce is disabled at index level. but gets overidden by the amount field



	-	Introduction to doc_values
		-	Elasticsearch makes use of several data structures.
			-	No single datastructure serves all purposes.
		-	Inverted indices are excellent for searching text.
			-	They don't perform well for many other data access patterns.
		-	"Doc values" is another data structure used by Apache lucene.
			-	Optimized for a different data access pattern (document-> terms)
			-	inverted index does not perform well in case of sorting or aggregating results because the access pattern is different. Instead doc_values perform well in this case.
			-	Instead of looking terms and finding the documents that contain them, we need to look up the document and find its terms for a field.

		-	doc_values is opposite of inverted index, i.e. an uninverted inverted index.
		-	Used for sorting, aggregations and scripting.
		-	It is an additional data structure and not a replacement for inverted index. Both can be applied to a field at the same time.
		-	Elasticsearch automatically queries the appropriate data structure.
		-	Disabling doc_values 
			-	Set the doc_values parameter to false to save disk space.
			-	Storing data in multiple data structures effectively duplicates data with the purpose of fast retrieval
				-	Also slightly increasing the index throughput
			-	Only disable doc values if you won't use aggregations, sorting and scripting.
			-	Particulary useful for large indices, typically not worth it for small ones.
			-	Cannot be changed without reindexing documents into new index.
				-	Use with caution, and try to anticipate how fields will be queried
			-	Example
				PUT /sales
				{
					"mappings" : {
						"properties" : {
							"buyer_email" : {
								"type" : "keyword",
								"doc_values": false
							}
						}
					}
				}
	-	Norms parmeter
		-	Normalization factors used for relevance scoring.
		-	Often we don't just want to filter results, but also rank them based on how well they match a given query. Think of search results on Google, for instance.
		-	Norms can be disabled to save disk space. Because they also take a lot of disk space like doc_values.
			-	Useful for fields that won't be used for relevance scoring.
			-	If disabled then also fields can be used for filtering and aggregations.
			-	Example
				PUT /sales
				{
					"mappings" : {
						"properties" : {
							"tags" : {
								"type" : "text",
								"norms" : false
							}
						}
					}
				}
	-	index parameter
		-	Disables indexing for a field.
		-	Values are still stored within _source object. It is just that the field value won't be part for the data structures used for searching.
		-	Useful if you won't use a field for search queries
		-	Saves disk space and slightly improves indexing throughtput.
		-	Often used for time series data
		-	Fields with indexing disabled can still be used for aggregations.

	-	null_value parameter
		-	null values cannot be indexed or searched.
		-	Use this parameter to replace NULL values with another value so that the field value can be searched.
		-	Only works for explicit NULL values. It won't work for example array field with empty array.
		-	The replacement value must be of the same data type as the field.
		-	Does not affect the value stored within _source.
		-	PUT /sales
			{
				"mappings" : {
					"properties" : {
						"paramter_id" : {
							"type" : "keyword",
							"null_value" : "NULL"
						}
					}
				}
			}
	-	copy_to parameter
		-	Used to copy multiple field values into a "group field". If a doc has two field first_name and last_name and we want to query for the full name. In that case we can copy the two fields into a single field full_name using copy_to.
		-	Simply specify the name of the target field as the value.
		-	E.g first_name and last_name -> full_name
		-	Values are copied, not terms/tokens
			-	The analyzer of the target field is used for the values.
		-	The target field is not a part of _source
		-	Example
			PUT /sales
			{
				"mappings" : {
					"properties" : {
						"first_name": {
							"type" : "text",
							"copy_to" : "full_name"
						},
						"last_name" : {
							"type" : "text",
							"copy_to" : "full_name"
						},
						"full_name" : {
							"type" : "text"
						}
					}
				}
			}

			POST /sales/_doc
			{
				"first_name" : "John",
				"last_name" : "Doe"
			}

Updating Existing Mappings
	-	Suppose that product IDs may now include letters.
	-	We need to change the product_id field's data type to either text or keyword
		-	We won't use the field for full-text searches
		-	We will use it for filtering, so the keyword datatype is ideal.
	-	If we try to update the mapping which is already present using the below query then we get the following message.
		-	PUT /reviews/_mapping
			{
				"properties" : {
					"product_id" : {
						"type" : "keyword"
					}
				}
			}
		-	It will give illegal_argument_exception.
	-	Limitations for updating mappings
		-	Generally, Elasticsearch field mappings cannot be changed.
		-	We can add new field mappings, but that's about it.
		-	A few mapping parameters can be updated for existing mappings.
		-	Example
			PUT /reviews/mapping
			{
				"properties" : {
					"author" : {
						"properties" : {
							"email" : {
								"type" : "keyword",
								"ignore_above" : 256
							}
						}
					}
				}
			}
			The above query will work which will update the existing field mapping for email. It will update such that email greater than 256 letters will be ignored and won't be stored or be a part for searching.
		-	Being able to update mappings would be problematic for existing documents
			-	Text values have already been analyzed, for instance
			-	Changing between some datatypes would require rebuilding the whole data structure.
		-	Even for an empty index, we cannot update a mapping.
		-	Field mappings also cannot be removed.
			-	Just leave out the field when indexing documents.
		-	The Update By Query API can be used to reclaim disk space.
		-	The solution is to reindex documents into a new index.


Reindex documents with the Reindex API
	-	GET /reviews/_mappings
		-	this will retreive the mappings for the given index.
	-	copy the mappings object and then paste it to the query
	-	changing the keyword datatype from long to keyword.
	-	PUT /reviews_new/
		{
		  "mappings": {
		    "properties": {
		      "author": {
		        "properties": {
		          "email": {
		            "type": "text",
		            "fields": {
		              "keyword": {
		                "type": "keyword",
		                "ignore_above": 256
		              }
		            }
		          },
		          "first_name": {
		            "type": "text",
		            "fields": {
		              "keyword": {
		                "type": "keyword",
		                "ignore_above": 256
		              }
		            }
		          },
		          "last_name": {
		            "type": "text",
		            "fields": {
		              "keyword": {
		                "type": "keyword",
		                "ignore_above": 256
		              }
		            }
		          }
		        }
		      },
		      "content": {
		        "type": "text",
		        "fields": {
		          "keyword": {
		            "type": "keyword",
		            "ignore_above": 256
		          }
		        }
		      },
		      "created_at": {
		        "type": "date"
		      },
		      "product_id": {
		        "type": "keyword"
		      },
		      "rating": {
		        "type": "float"
		      }
		    }
		  }
		}

	-	Now to transfer the documents to the new index ElasticSearch exposes a api called as 
		_reindex api.
	-	POST /_reindex
		{
			"source" : {
				"index" : "reviews"
			},
			"dest" : {
				"index" : "reviews_new"
			}
		}
	-	Now to search for documents
		GET /reviews_new/_search
		{
		  "query" : {
		    "match_all" : {}
		  }
		}
	-	But, if we check the docs in the new index we will see that, product_id is present in the form of long or integer and not string as we added the keyword datatype.
	-	_source datatypes
		-	The data type doesn't reflect how the values are indexed.
		-	_source contains the field original values supplied at index time. In our case it was integer or long. Us changing the data type does not affect the "_source" object to be modified.
		-	It's common to use _source values from search results.
			-	You would probably expect a string for a keyword field.
		-	We can modify the _source value while reindexing.
		-	Alternatively this can be handled at application level.
		-	To update the datatype for the fields inside the source object a script can be written. But before that docs needs to be deleted from the new index.
		-	POST /reviews_new/_delete_by_query
			{
				"query" : {
					"match_all" : {

					}
				}
			}
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews"
				},
				"dest" : {
					"index" : "reviews_new"
				}
				"script" : {
					"source" : """
						if (ctx._source.product_id != null) {
							ctx._source.product_id = ctx._source.product_id.toString();
						}
					"""
				}
			}
		-	If now if you check then the fields inside the _source object has converted from integer/long to string.

	-	Reindex documents matching a query
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews",
					"query" : {
						"match_all" : {}
					}
				},
				"dest" : {
					"index": "reviews_new"
				}
			}
	-	Reindex only positive reviews
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews",
					"query" : {
						"range" : {
							"rating" : {
								"gte" : 4.0
							}
						}
					}
				},
				"dest" : {
					"index" : "reviews_new"
				}
			}
	-	Removing fields
		-	Field mappings cannot be deleted.
		-	Fields can be left out when indexing documents.
		-	Maybe we want to reclaim disk space used by a field.
			-	Already indexed values still take up disk space.
			-	For large data sets, this maybe worthwhile
				-	Assuming that we no longer need the values.
		-	This can be done using the source filtering.
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews",
					"_source" : ["content", "created_at", "rating"]
				},
				"dest" : {
					"index" : "reviews_new"
				}
			}
		-	By specifying an array of field names, only those fields are included for each document when they are indexed into the destination index. In other words, any fields that you leave out will not be reindexed.
	-	Changing a field's name
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews"
				}
				"dest" : {
					"index" : "reviews_new"
				},
				"script" : {
					"source" : """
						#Rename "content" field to "comment"
						ctx._source.comment = ctx._source.remove("content");
					"""
				}
			}
	-	Ignore reviews with ratings below 4.0
		-	POST /_reindex
			{
				"source" : {
					"index" : "reviews"
				},
				"dest" : {
					"index" : "reviews_new"
				},
				"script" : {
					"source" : """
						if (ctx._source.rating < 4.0) {
							ctx.op = "noop"; # Can also be set to "delete"
						}
					"""
				}
			}
		-	In case of noop document will not be indexed to the destination index
		-	In case of delete document will get deleted from the destination index.
			-	The destination index might not be empty as in our example. So the document may be present in the destination index. It will get deleted if used "delete".

-	Batching and Throttling
	-	The Reindex API performs operations in batches
		-	Just like the Update by Query and Delete by Query APIs.
		-	It uses the scroll API internally.
		-	This is how millions of documents can be reindexed efficiently.
	-	Throttling can be configured to limit the perfomance impact.
	-	Check documentation if you need to reindex lots of documents.


            
   





        
        


